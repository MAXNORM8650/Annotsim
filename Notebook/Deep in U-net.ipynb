{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78bb6eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import copy\n",
    "import sys\n",
    "import time\n",
    "from random import seed\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib import animation\n",
    "from torch import optim\n",
    "\n",
    "import dataset\n",
    "import evaluation\n",
    "from GaussianDiffusion import GaussianDiffusionModel, get_beta_schedule\n",
    "from helpers import *\n",
    "# from UNet import UNetModel, update_ema_params\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "ROOT_DIR = \"./\"\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd95df3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "  \"img_size\": [\n",
    "    256,\n",
    "    256\n",
    "  ],\n",
    "  \"Batch_Size\": 16,\n",
    "  \"EPOCHS\": 3000,\n",
    "  \"T\": 1000,\n",
    "  \"base_channels\": 128,\n",
    "  \"beta_schedule\": \"linear\",\n",
    "  \"channel_mults\": \"\",\n",
    "  \"loss-type\": \"l2\",\n",
    "  \"loss_weight\": \"none\",\n",
    "  \"train_start\": True,\n",
    "  \"lr\": 1e-4,\n",
    "  \"random_slice\": False,\n",
    "  \"sample_distance\": 800,\n",
    "  \"weight_decay\": 0.0,\n",
    "  \"save_imgs\": False,\n",
    "  \"save_vids\": True,\n",
    "  \"dropout\": 0,\n",
    "  \"attention_resolutions\": \"16,8\",\n",
    "  \"num_heads\": 2,\n",
    "  \"num_head_channels\": -1,\n",
    "  \"noise_fn\": \"simplex\",\n",
    "  \"dataset\": \"mri\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9dac9194",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from abc import abstractmethod\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class TimestepBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Any module where forward() takes timestep embeddings as a second argument.\n",
    "    \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self, x, emb):\n",
    "        \"\"\"\n",
    "        Apply the module to `x` given `emb` timestep embeddings.\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "class TimestepEmbedSequential(nn.Sequential, TimestepBlock):\n",
    "    \"\"\"\n",
    "    A sequential module that passes timestep embeddings to the children that\n",
    "    support it as an extra input.\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, x, emb):\n",
    "        for layer in self:\n",
    "            if isinstance(layer, TimestepBlock):\n",
    "                x = layer(x, emb)\n",
    "            else:\n",
    "                x = layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class PositionalEmbedding(nn.Module):\n",
    "    # PositionalEmbedding\n",
    "    \"\"\"\n",
    "    Computes Positional Embedding of the timestep\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, scale=1):\n",
    "        super().__init__()\n",
    "        assert dim % 2 == 0\n",
    "        self.dim = dim\n",
    "        self.scale = scale\n",
    "\n",
    "    def forward(self, x):\n",
    "        device = x.device\n",
    "        half_dim = self.dim // 2\n",
    "        emb = np.log(10000) / half_dim\n",
    "        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
    "        emb = torch.outer(x * self.scale, emb)\n",
    "        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
    "        return emb\n",
    "\n",
    "\n",
    "class Downsample(nn.Module):\n",
    "    def __init__(self, in_channels, use_conv, out_channels=None):\n",
    "        super().__init__()\n",
    "        self.channels = in_channels\n",
    "        out_channels = out_channels or in_channels\n",
    "        if use_conv:\n",
    "            # downsamples by 1/2\n",
    "            self.downsample = nn.Conv2d(in_channels, out_channels, 3, stride=2, padding=1)\n",
    "        else:\n",
    "            assert in_channels == out_channels\n",
    "            self.downsample = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self, x, time_embed=None):\n",
    "        assert x.shape[1] == self.channels\n",
    "        return self.downsample(x)\n",
    "\n",
    "\n",
    "class Upsample(nn.Module):\n",
    "    def __init__(self, in_channels, use_conv, out_channels=None):\n",
    "        super().__init__()\n",
    "        self.channels = in_channels\n",
    "        self.use_conv = use_conv\n",
    "        # uses upsample then conv to avoid checkerboard artifacts\n",
    "        # self.upsample = nn.Upsample(scale_factor=2, mode=\"nearest\")\n",
    "        if use_conv:\n",
    "            self.conv = nn.Conv2d(in_channels, out_channels, 3, padding=1)\n",
    "\n",
    "    def forward(self, x, time_embed=None):\n",
    "        assert x.shape[1] == self.channels\n",
    "        x = F.interpolate(x, scale_factor=2, mode=\"nearest\")\n",
    "        if self.use_conv:\n",
    "            x = self.conv(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "622c8fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    An attention block that allows spatial positions to attend to each other.\n",
    "    Originally ported from here, but adapted to the N-d case.\n",
    "    https://github.com/hojonathanho/diffusion/blob/1e0dceb3b3495bbe19116a5e1b3596cd0706c543/diffusion_tf/models/unet.py#L66.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, n_heads=1, n_head_channels=-1):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.norm = GroupNorm32(32, self.in_channels)\n",
    "        if n_head_channels == -1:\n",
    "            self.num_heads = n_heads\n",
    "        else:\n",
    "            assert (\n",
    "                    in_channels % n_head_channels == 0\n",
    "            ), f\"q,k,v channels {in_channels} is not divisible by num_head_channels {n_head_channels}\"\n",
    "            self.num_heads = in_channels // n_head_channels\n",
    "\n",
    "        # query, key, value for attention\n",
    "        self.to_qkv = nn.Conv1d(in_channels, in_channels * 3, 1)\n",
    "        self.attention = QKVAttention(self.num_heads)\n",
    "        self.proj_out = zero_module(nn.Conv1d(in_channels, in_channels, 1))\n",
    "\n",
    "    def forward(self, x, time=None):\n",
    "        b, c, *spatial = x.shape\n",
    "        x = x.reshape(b, c, -1)\n",
    "        qkv = self.to_qkv(self.norm(x))\n",
    "        h = self.attention(qkv)\n",
    "        h = self.proj_out(h)\n",
    "        return (x + h).reshape(b, c, *spatial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8354a730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AB = AttentionBlock(512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e0e731f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = torch.rand(1,  512, 7, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fec61d9c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AB' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m y_test \u001b[38;5;241m=\u001b[39m \u001b[43mAB\u001b[49m(x_test)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'AB' is not defined"
     ]
    }
   ],
   "source": [
    "y_test = AB(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "901b908b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "16b88f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class QKVAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    A module which performs QKV attention. Matches legacy QKVAttention + input/ouput heads shaping\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_heads):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "    def forward(self, qkv, time=None):\n",
    "        \"\"\"\n",
    "        Apply QKV attention.\n",
    "        :param qkv: an [N x (H * 3 * C) x T] tensor of Qs, Ks, and Vs.\n",
    "        :return: an [N x (H * C) x T] tensor after attention.\n",
    "        \"\"\"\n",
    "        bs, width, length = qkv.shape\n",
    "        assert width % (3 * self.n_heads) == 0\n",
    "        ch = width // (3 * self.n_heads)\n",
    "        q, k, v = qkv.reshape(bs * self.n_heads, ch * 3, length).split(ch, dim=1)\n",
    "        scale = 1 / math.sqrt(math.sqrt(ch))\n",
    "        weight = torch.einsum(\n",
    "                \"bct,bcs->bts\", q * scale, k * scale\n",
    "                )  # More stable with f16 than dividing afterwards\n",
    "        weight = torch.softmax(weight.float(), dim=-1).type(weight.dtype)\n",
    "        a = torch.einsum(\"bts,bcs->bct\", weight, v)\n",
    "        return a.reshape(bs, -1, length)\n",
    "\n",
    "\n",
    "class ResBlock(TimestepBlock):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels,\n",
    "            time_embed_dim,\n",
    "            dropout,\n",
    "            out_channels=None,\n",
    "            use_conv=False,\n",
    "            up=False,\n",
    "            down=False\n",
    "            ):\n",
    "        super().__init__()\n",
    "        out_channels = out_channels or in_channels\n",
    "        self.in_layers = nn.Sequential(\n",
    "                GroupNorm32(32, in_channels),\n",
    "                nn.SiLU(),\n",
    "                nn.Conv2d(in_channels, out_channels, 3, padding=1)\n",
    "                )\n",
    "        self.updown = up or down\n",
    "\n",
    "        if up:\n",
    "            self.h_upd = Upsample(in_channels, False)\n",
    "            self.x_upd = Upsample(in_channels, False)\n",
    "        elif down:\n",
    "            self.h_upd = Downsample(in_channels, False)\n",
    "            self.x_upd = Downsample(in_channels, False)\n",
    "        else:\n",
    "            self.h_upd = self.x_upd = nn.Identity()\n",
    "\n",
    "        self.embed_layers = nn.Sequential(\n",
    "                nn.SiLU(),\n",
    "                nn.Linear(time_embed_dim, out_channels)\n",
    "                )\n",
    "        self.out_layers = nn.Sequential(\n",
    "                GroupNorm32(32, out_channels),\n",
    "                nn.SiLU(),\n",
    "                nn.Dropout(p=dropout),\n",
    "                zero_module(nn.Conv2d(out_channels, out_channels, 3, padding=1))\n",
    "                )\n",
    "        if out_channels == in_channels:\n",
    "            self.skip_connection = nn.Identity()\n",
    "        elif use_conv:\n",
    "            self.skip_connection = nn.Conv2d(in_channels, out_channels, 3, padding=1)\n",
    "        else:\n",
    "            self.skip_connection = nn.Conv2d(in_channels, out_channels, 1)\n",
    "\n",
    "    def forward(self, x, time_embed):\n",
    "        if self.updown:\n",
    "            in_rest, in_conv = self.in_layers[:-1], self.in_layers[-1]\n",
    "            h = in_rest(x)\n",
    "            h = self.h_upd(h)\n",
    "            x = self.x_upd(x)\n",
    "            h = in_conv(h)\n",
    "        else:\n",
    "            h = self.in_layers(x)\n",
    "        emb_out = self.embed_layers(time_embed).type(h.dtype)\n",
    "        while len(emb_out.shape) < len(h.shape):\n",
    "            emb_out = emb_out[..., None]\n",
    "\n",
    "        h = h + emb_out\n",
    "        h = self.out_layers(h)\n",
    "        return self.skip_connection(x) + h\n",
    "\n",
    "\n",
    "class UNetModel(nn.Module):\n",
    "    # UNet model\n",
    "    def __init__(\n",
    "            self,\n",
    "            img_size,\n",
    "            base_channels,\n",
    "            conv_resample=True,\n",
    "            n_heads=1,\n",
    "            n_head_channels=-1,\n",
    "            channel_mults=\"\",\n",
    "            num_res_blocks=2,\n",
    "            dropout=0,\n",
    "            attention_resolutions=\"32,16,8\",\n",
    "            biggan_updown=True,\n",
    "            in_channels=1\n",
    "            ):\n",
    "        self.dtype = torch.float32\n",
    "        super().__init__()\n",
    "\n",
    "        if channel_mults == \"\":\n",
    "            if img_size == 512:\n",
    "                channel_mults = (0.5, 1, 1, 2, 2, 4, 4)\n",
    "            elif img_size == 256:\n",
    "                channel_mults = (1, 1, 2, 2, 4, 4)\n",
    "            elif img_size == 128:\n",
    "                channel_mults = (1, 1, 2, 3, 4)\n",
    "            elif img_size == 64:\n",
    "                channel_mults = (1, 2, 3, 4)\n",
    "            elif img_size == 32:\n",
    "                channel_mults = (1, 2, 3, 4)\n",
    "            else:\n",
    "                raise ValueError(f\"unsupported image size: {img_size}\")\n",
    "        attention_ds = []\n",
    "        for res in attention_resolutions.split(\",\"):\n",
    "            attention_ds.append(img_size // int(res))\n",
    "\n",
    "        self.image_size = img_size\n",
    "        self.in_channels = in_channels\n",
    "        self.model_channels = base_channels\n",
    "        self.out_channels = in_channels\n",
    "        self.num_res_blocks = num_res_blocks\n",
    "        self.attention_resolutions = attention_resolutions\n",
    "        self.dropout = dropout\n",
    "        self.channel_mult = channel_mults\n",
    "        self.conv_resample = conv_resample\n",
    "\n",
    "        self.dtype = torch.float32\n",
    "        self.num_heads = n_heads\n",
    "        self.num_head_channels = n_head_channels\n",
    "\n",
    "        time_embed_dim = base_channels * 4\n",
    "        self.time_embedding = nn.Sequential(\n",
    "                PositionalEmbedding(base_channels, 1),\n",
    "                nn.Linear(base_channels, time_embed_dim),\n",
    "                nn.SiLU(),\n",
    "                nn.Linear(time_embed_dim, time_embed_dim),\n",
    "                )\n",
    "\n",
    "        ch = int(channel_mults[0] * base_channels)\n",
    "        self.down = nn.ModuleList(\n",
    "                [TimestepEmbedSequential(nn.Conv2d(self.in_channels, base_channels, 3, padding=1))]\n",
    "                )\n",
    "        channels = [ch]\n",
    "        ds = 1\n",
    "        for i, mult in enumerate(channel_mults):\n",
    "            # out_channels = base_channels * mult\n",
    "\n",
    "            for _ in range(num_res_blocks):\n",
    "                layers = [ResBlock(\n",
    "                        ch,\n",
    "                        time_embed_dim=time_embed_dim,\n",
    "                        out_channels=base_channels * mult,\n",
    "                        dropout=dropout,\n",
    "                        )]\n",
    "                ch = base_channels * mult\n",
    "                # channels.append(ch)\n",
    "\n",
    "                if ds in attention_ds:\n",
    "                    layers.append(\n",
    "                            AttentionBlock(\n",
    "                                    ch,\n",
    "                                    n_heads=n_heads,\n",
    "                                    n_head_channels=n_head_channels,\n",
    "                                    )\n",
    "                            )\n",
    "                self.down.append(TimestepEmbedSequential(*layers))\n",
    "                channels.append(ch)\n",
    "            if i != len(channel_mults) - 1:\n",
    "                out_channels = ch\n",
    "                self.down.append(\n",
    "                        TimestepEmbedSequential(\n",
    "                                ResBlock(\n",
    "                                        ch,\n",
    "                                        time_embed_dim=time_embed_dim,\n",
    "                                        out_channels=out_channels,\n",
    "                                        dropout=dropout,\n",
    "                                        down=True\n",
    "                                        )\n",
    "                                if biggan_updown\n",
    "                                else\n",
    "                                Downsample(ch, conv_resample, out_channels=out_channels)\n",
    "                                )\n",
    "                        )\n",
    "                ds *= 2\n",
    "                ch = out_channels\n",
    "                channels.append(ch)\n",
    "#         self.MaskBlock = SSPCAB(ch, reduction_ratio=8)\n",
    "        self.middle = TimestepEmbedSequential(\n",
    "                ResBlock(\n",
    "                        ch,\n",
    "                        time_embed_dim=time_embed_dim,\n",
    "                        dropout=dropout\n",
    "                        ),\n",
    "                SSPCAB(\n",
    "                      ch,\n",
    "                      reduction_ratio=8\n",
    "                      ),\n",
    "                ResBlock(\n",
    "                        ch,\n",
    "                        time_embed_dim=time_embed_dim,\n",
    "                        dropout=dropout\n",
    "                        )\n",
    "                )\n",
    "        self.up = nn.ModuleList([])\n",
    "\n",
    "        for i, mult in reversed(list(enumerate(channel_mults))):\n",
    "            for j in range(num_res_blocks + 1):\n",
    "                inp_chs = channels.pop()\n",
    "                layers = [\n",
    "                    ResBlock(\n",
    "                            ch + inp_chs,\n",
    "                            time_embed_dim=time_embed_dim,\n",
    "                            out_channels=base_channels * mult,\n",
    "                            dropout=dropout\n",
    "                            )\n",
    "                    ]\n",
    "                ch = base_channels * mult\n",
    "                if ds in attention_ds:\n",
    "                    layers.append(\n",
    "                            AttentionBlock(\n",
    "                                    ch,\n",
    "                                    n_heads=n_heads,\n",
    "                                    n_head_channels=n_head_channels\n",
    "                                    ),\n",
    "                            )\n",
    "\n",
    "                if i and j == num_res_blocks:\n",
    "                    out_channels = ch\n",
    "                    layers.append(\n",
    "                            ResBlock(\n",
    "                                    ch,\n",
    "                                    time_embed_dim=time_embed_dim,\n",
    "                                    out_channels=out_channels,\n",
    "                                    dropout=dropout,\n",
    "                                    up=True\n",
    "                                    )\n",
    "                            if biggan_updown\n",
    "                            else\n",
    "                            Upsample(ch, conv_resample, out_channels=out_channels)\n",
    "                            )\n",
    "                    ds //= 2\n",
    "                self.up.append(TimestepEmbedSequential(*layers))\n",
    "\n",
    "        self.out = nn.Sequential(\n",
    "                GroupNorm32(32, ch),\n",
    "                nn.SiLU(),\n",
    "                zero_module(nn.Conv2d(base_channels * channel_mults[0], self.out_channels, 3, padding=1))\n",
    "                )\n",
    "\n",
    "    def forward(self, x, time):\n",
    "\n",
    "        time_embed = self.time_embedding(time)\n",
    "        skips = []\n",
    "\n",
    "        h = x.type(self.dtype)\n",
    "        for i, module in enumerate(self.down):\n",
    "            h = module(h, time_embed)\n",
    "            skips.append(h)\n",
    "        \n",
    "        fm = h\n",
    "#         print(fm.shape)\n",
    "        h_hat = self.middle(fm, time_embed)\n",
    "        H = h_hat\n",
    "#         print(h_hat.shape)\n",
    "        for i, module in enumerate(self.up):\n",
    "            H = torch.cat([H, skips.pop()], dim=1)\n",
    "            H = module(H, time_embed)\n",
    "        H = H.type(x.dtype)\n",
    "        H = self.out(H)\n",
    "        return H, fm, h_hat\n",
    "\n",
    "\n",
    "class GroupNorm32(nn.GroupNorm):\n",
    "    def forward(self, x):\n",
    "        return super().forward(x.float()).type(x.dtype)\n",
    "\n",
    "\n",
    "def zero_module(module):\n",
    "    \"\"\"\n",
    "    Zero out the parameters of a module and return it.\n",
    "    \"\"\"\n",
    "    for p in module.parameters():\n",
    "        p.detach().zero_()\n",
    "    return module\n",
    "\n",
    "\n",
    "def update_ema_params(target, source, decay_rate=0.9999):\n",
    "    targParams = dict(target.named_parameters())\n",
    "    srcParams = dict(source.named_parameters())\n",
    "    for k in targParams:\n",
    "        targParams[k].data.mul_(decay_rate).add_(srcParams[k].data, alpha=1 - decay_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0dab65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d455ba58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3561bcaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "013c45c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Squeeze and Excitation block\n",
    "class SELayer(nn.Module):\n",
    "    def __init__(self, num_channels, reduction_ratio=8):\n",
    "        '''\n",
    "            num_channels: The number of input channels\n",
    "            reduction_ratio: The reduction ratio 'r' from the paper\n",
    "        '''\n",
    "        super(SELayer, self).__init__()\n",
    "        num_channels_reduced = num_channels // reduction_ratio\n",
    "        self.reduction_ratio = reduction_ratio\n",
    "        self.fc1 = nn.Linear(num_channels, num_channels_reduced, bias=True)\n",
    "        self.fc2 = nn.Linear(num_channels_reduced, num_channels, bias=True)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        batch_size, num_channels, H, W = input_tensor.size()\n",
    "\n",
    "        squeeze_tensor = input_tensor.view(batch_size, num_channels, -1).mean(dim=2)\n",
    "#         print(squeeze_tensor.shape)\n",
    "        # channel excitation\n",
    "        fc_out_1 = self.relu(self.fc1(squeeze_tensor))\n",
    "#         print(fc_out_1.shape)\n",
    "        fc_out_2 = self.sigmoid(self.fc2(fc_out_1))\n",
    "#         print(fc_out_2.shape)\n",
    "        a, b = squeeze_tensor.size()\n",
    "#         print(a,b)\n",
    "#         fc_out_2 = torch.reshape(fc_out_2, a, b, 1, 1)\n",
    "        output_tensor = torch.mul(input_tensor, fc_out_2.view(a, b, 1, 1))\n",
    "        return output_tensor\n",
    "\n",
    "\n",
    "# SSPCAB implementation\n",
    "class SSPCAB(nn.Module):\n",
    "    def __init__(self, channels, kernel_dim=1, dilation=1, reduction_ratio=4):\n",
    "        '''\n",
    "            channels: The number of filter at the output (usually the same with the number of filter from the input)\n",
    "            kernel_dim: The dimension of the sub-kernels ' k' ' from the paper\n",
    "            dilation: The dilation dimension 'd' from the paper\n",
    "            reduction_ratio: The reduction ratio for the SE block ('r' from the paper)\n",
    "        '''\n",
    "        super(SSPCAB, self).__init__()\n",
    "        self.pad = kernel_dim + dilation\n",
    "        self.border_input = kernel_dim + 2*dilation + 1\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.se = SELayer(channels, reduction_ratio=reduction_ratio)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=channels,\n",
    "                               out_channels=channels,\n",
    "                               kernel_size=kernel_dim)\n",
    "        self.conv2 = nn.Conv2d(in_channels=channels,\n",
    "                               out_channels=channels,\n",
    "                               kernel_size=kernel_dim)\n",
    "        self.conv3 = nn.Conv2d(in_channels=channels,\n",
    "                               out_channels=channels,\n",
    "                               kernel_size=kernel_dim)\n",
    "        self.conv4 = nn.Conv2d(in_channels=channels,\n",
    "                               out_channels=channels,\n",
    "                               kernel_size=kernel_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.pad(x, (self.pad, self.pad, self.pad, self.pad), \"constant\", 0)\n",
    "\n",
    "        x1 = self.conv1(x[:, :, :-self.border_input, :-self.border_input])\n",
    "        x2 = self.conv2(x[:, :, self.border_input:, :-self.border_input])\n",
    "        x3 = self.conv3(x[:, :, :-self.border_input, self.border_input:])\n",
    "        x4 = self.conv4(x[:, :, self.border_input:, self.border_input:])\n",
    "        x = self.relu(x1 + x2 + x3 + x4)\n",
    "\n",
    "        x = self.se(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6e6bff92",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = torch.rand(1, 3, 448, 448)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cb7ede5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = SSPCAB(3)(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1a61069b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 448, 448])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "385b2402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256, 16, 16])\n",
      "torch.Size([1, 256, 16, 16])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [78]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m512\u001b[39m, \u001b[38;5;241m512\u001b[39m)\n\u001b[0;32m     18\u001b[0m t_batch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m1\u001b[39m], device\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mrepeat(x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_batch\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    args = {\n",
    "        'img_size':          256,\n",
    "        'base_channels':     64,\n",
    "        'dropout':           0.3,\n",
    "        'num_heads':         4,\n",
    "        'num_head_channels': '32,16,8',\n",
    "        'lr':                1e-4,\n",
    "        'Batch_Size':        64\n",
    "        }\n",
    "    model = UNetModel(\n",
    "            args['img_size'], args['base_channels'], dropout=args[\n",
    "                \"dropout\"], n_heads=args[\"num_heads\"], attention_resolutions=args[\"num_head_channels\"],\n",
    "            in_channels=3\n",
    "            )\n",
    "\n",
    "    x = torch.randn(1, 3, 512, 512)\n",
    "    t_batch = torch.tensor([1], device=x.device).repeat(x.shape[0])\n",
    "#     print(model(x, t_batch).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "60180fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MIDDLE = model.middle(x, t_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "bd2c9eb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'img_size': [256, 256],\n",
       " 'Batch_Size': 16,\n",
       " 'EPOCHS': 3000,\n",
       " 'T': 1000,\n",
       " 'base_channels': 128,\n",
       " 'beta_schedule': 'linear',\n",
       " 'channel_mults': '',\n",
       " 'loss-type': 'l2',\n",
       " 'loss_weight': 'none',\n",
       " 'train_start': True,\n",
       " 'lr': 0.0001,\n",
       " 'random_slice': False,\n",
       " 'sample_distance': 800,\n",
       " 'weight_decay': 0.0,\n",
       " 'save_imgs': False,\n",
       " 'save_vids': True,\n",
       " 'dropout': 0,\n",
       " 'attention_resolutions': '16,8',\n",
       " 'num_heads': 2,\n",
       " 'num_head_channels': -1,\n",
       " 'noise_fn': 'simplex',\n",
       " 'dataset': 'mri'}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a4c79c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_channels=3\n",
    "unet = UNetModel(\n",
    "        args['img_size'][0], args['base_channels'], channel_mults=args['channel_mults'], in_channels=in_channels\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f21f6484",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UNetModel(\n",
       "  (time_embedding): Sequential(\n",
       "    (0): PositionalEmbedding()\n",
       "    (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "    (2): SiLU()\n",
       "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "  )\n",
       "  (down): ModuleList(\n",
       "    (0): TimestepEmbedSequential(\n",
       "      (0): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (1): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 128, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (embed_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=512, out_features=128, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 128, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Identity()\n",
       "      )\n",
       "    )\n",
       "    (2): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 128, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (embed_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=512, out_features=128, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 128, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Identity()\n",
       "      )\n",
       "    )\n",
       "    (3): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 128, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Downsample(\n",
       "          (downsample): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "        )\n",
       "        (x_upd): Downsample(\n",
       "          (downsample): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "        )\n",
       "        (embed_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=512, out_features=128, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 128, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Identity()\n",
       "      )\n",
       "    )\n",
       "    (4): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 128, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (embed_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=512, out_features=128, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 128, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Identity()\n",
       "      )\n",
       "    )\n",
       "    (5): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 128, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (embed_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=512, out_features=128, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 128, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Identity()\n",
       "      )\n",
       "    )\n",
       "    (6): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 128, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Downsample(\n",
       "          (downsample): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "        )\n",
       "        (x_upd): Downsample(\n",
       "          (downsample): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "        )\n",
       "        (embed_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=512, out_features=128, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 128, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Identity()\n",
       "      )\n",
       "    )\n",
       "    (7): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 128, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (embed_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=512, out_features=256, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (8): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (embed_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=512, out_features=256, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Identity()\n",
       "      )\n",
       "    )\n",
       "    (9): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Downsample(\n",
       "          (downsample): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "        )\n",
       "        (x_upd): Downsample(\n",
       "          (downsample): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "        )\n",
       "        (embed_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=512, out_features=256, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Identity()\n",
       "      )\n",
       "    )\n",
       "    (10): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (embed_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=512, out_features=256, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Identity()\n",
       "      )\n",
       "      (1): AttentionBlock(\n",
       "        (norm): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
       "        (to_qkv): Conv1d(256, 768, kernel_size=(1,), stride=(1,))\n",
       "        (attention): QKVAttention()\n",
       "        (proj_out): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "    )\n",
       "    (11): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (embed_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=512, out_features=256, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Identity()\n",
       "      )\n",
       "      (1): AttentionBlock(\n",
       "        (norm): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
       "        (to_qkv): Conv1d(256, 768, kernel_size=(1,), stride=(1,))\n",
       "        (attention): QKVAttention()\n",
       "        (proj_out): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "    )\n",
       "    (12): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Downsample(\n",
       "          (downsample): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "        )\n",
       "        (x_upd): Downsample(\n",
       "          (downsample): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "        )\n",
       "        (embed_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=512, out_features=256, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Identity()\n",
       "      )\n",
       "    )\n",
       "    (13): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (embed_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 512, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (1): AttentionBlock(\n",
       "        (norm): GroupNorm32(32, 512, eps=1e-05, affine=True)\n",
       "        (to_qkv): Conv1d(512, 1536, kernel_size=(1,), stride=(1,))\n",
       "        (attention): QKVAttention()\n",
       "        (proj_out): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "    )\n",
       "    (14): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 512, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (embed_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 512, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Identity()\n",
       "      )\n",
       "      (1): AttentionBlock(\n",
       "        (norm): GroupNorm32(32, 512, eps=1e-05, affine=True)\n",
       "        (to_qkv): Conv1d(512, 1536, kernel_size=(1,), stride=(1,))\n",
       "        (attention): QKVAttention()\n",
       "        (proj_out): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "    )\n",
       "    (15): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 512, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Downsample(\n",
       "          (downsample): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "        )\n",
       "        (x_upd): Downsample(\n",
       "          (downsample): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "        )\n",
       "        (embed_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 512, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Identity()\n",
       "      )\n",
       "    )\n",
       "    (16): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 512, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (embed_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 512, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Identity()\n",
       "      )\n",
       "      (1): AttentionBlock(\n",
       "        (norm): GroupNorm32(32, 512, eps=1e-05, affine=True)\n",
       "        (to_qkv): Conv1d(512, 1536, kernel_size=(1,), stride=(1,))\n",
       "        (attention): QKVAttention()\n",
       "        (proj_out): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "    )\n",
       "    (17): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 512, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (embed_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 512, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Identity()\n",
       "      )\n",
       "      (1): AttentionBlock(\n",
       "        (norm): GroupNorm32(32, 512, eps=1e-05, affine=True)\n",
       "        (to_qkv): Conv1d(512, 1536, kernel_size=(1,), stride=(1,))\n",
       "        (attention): QKVAttention()\n",
       "        (proj_out): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (middle): TimestepEmbedSequential(\n",
       "    (0): ResBlock(\n",
       "      (in_layers): Sequential(\n",
       "        (0): GroupNorm32(32, 512, eps=1e-05, affine=True)\n",
       "        (1): SiLU()\n",
       "        (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (h_upd): Identity()\n",
       "      (x_upd): Identity()\n",
       "      (embed_layers): Sequential(\n",
       "        (0): SiLU()\n",
       "        (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (out_layers): Sequential(\n",
       "        (0): GroupNorm32(32, 512, eps=1e-05, affine=True)\n",
       "        (1): SiLU()\n",
       "        (2): Dropout(p=0, inplace=False)\n",
       "        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (skip_connection): Identity()\n",
       "    )\n",
       "    (1): SSPCAB(\n",
       "      (relu): ReLU()\n",
       "      (se): SELayer(\n",
       "        (fc1): Linear(in_features=512, out_features=64, bias=True)\n",
       "        (fc2): Linear(in_features=64, out_features=512, bias=True)\n",
       "        (relu): ReLU()\n",
       "        (sigmoid): Sigmoid()\n",
       "      )\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (conv3): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (conv4): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (2): ResBlock(\n",
       "      (in_layers): Sequential(\n",
       "        (0): GroupNorm32(32, 512, eps=1e-05, affine=True)\n",
       "        (1): SiLU()\n",
       "        (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (h_upd): Identity()\n",
       "      (x_upd): Identity()\n",
       "      (embed_layers): Sequential(\n",
       "        (0): SiLU()\n",
       "        (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (out_layers): Sequential(\n",
       "        (0): GroupNorm32(32, 512, eps=1e-05, affine=True)\n",
       "        (1): SiLU()\n",
       "        (2): Dropout(p=0, inplace=False)\n",
       "        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (skip_connection): Identity()\n",
       "    )\n",
       "  )\n",
       "  (up): ModuleList(\n",
       "    (0): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 1024, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (embed_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 512, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (1): AttentionBlock(\n",
       "        (norm): GroupNorm32(32, 512, eps=1e-05, affine=True)\n",
       "        (to_qkv): Conv1d(512, 1536, kernel_size=(1,), stride=(1,))\n",
       "        (attention): QKVAttention()\n",
       "        (proj_out): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "    )\n",
       "    (1): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 1024, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (embed_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 512, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (1): AttentionBlock(\n",
       "        (norm): GroupNorm32(32, 512, eps=1e-05, affine=True)\n",
       "        (to_qkv): Conv1d(512, 1536, kernel_size=(1,), stride=(1,))\n",
       "        (attention): QKVAttention()\n",
       "        (proj_out): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "    )\n",
       "    (2): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 1024, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (embed_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 512, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (1): AttentionBlock(\n",
       "        (norm): GroupNorm32(32, 512, eps=1e-05, affine=True)\n",
       "        (to_qkv): Conv1d(512, 1536, kernel_size=(1,), stride=(1,))\n",
       "        (attention): QKVAttention()\n",
       "        (proj_out): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (2): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 512, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Upsample()\n",
       "        (x_upd): Upsample()\n",
       "        (embed_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 512, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Identity()\n",
       "      )\n",
       "    )\n",
       "    (3): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 1024, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (embed_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 512, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (1): AttentionBlock(\n",
       "        (norm): GroupNorm32(32, 512, eps=1e-05, affine=True)\n",
       "        (to_qkv): Conv1d(512, 1536, kernel_size=(1,), stride=(1,))\n",
       "        (attention): QKVAttention()\n",
       "        (proj_out): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "    )\n",
       "    (4): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 1024, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (embed_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 512, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (1): AttentionBlock(\n",
       "        (norm): GroupNorm32(32, 512, eps=1e-05, affine=True)\n",
       "        (to_qkv): Conv1d(512, 1536, kernel_size=(1,), stride=(1,))\n",
       "        (attention): QKVAttention()\n",
       "        (proj_out): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "    )\n",
       "    (5): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 768, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(768, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (embed_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 512, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Conv2d(768, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (1): AttentionBlock(\n",
       "        (norm): GroupNorm32(32, 512, eps=1e-05, affine=True)\n",
       "        (to_qkv): Conv1d(512, 1536, kernel_size=(1,), stride=(1,))\n",
       "        (attention): QKVAttention()\n",
       "        (proj_out): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (2): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 512, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Upsample()\n",
       "        (x_upd): Upsample()\n",
       "        (embed_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 512, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Identity()\n",
       "      )\n",
       "    )\n",
       "    (6): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 768, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(768, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (embed_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=512, out_features=256, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (1): AttentionBlock(\n",
       "        (norm): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
       "        (to_qkv): Conv1d(256, 768, kernel_size=(1,), stride=(1,))\n",
       "        (attention): QKVAttention()\n",
       "        (proj_out): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "    )\n",
       "    (7): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 512, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (embed_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=512, out_features=256, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (1): AttentionBlock(\n",
       "        (norm): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
       "        (to_qkv): Conv1d(256, 768, kernel_size=(1,), stride=(1,))\n",
       "        (attention): QKVAttention()\n",
       "        (proj_out): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "    )\n",
       "    (8): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 512, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (embed_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=512, out_features=256, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (1): AttentionBlock(\n",
       "        (norm): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
       "        (to_qkv): Conv1d(256, 768, kernel_size=(1,), stride=(1,))\n",
       "        (attention): QKVAttention()\n",
       "        (proj_out): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (2): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Upsample()\n",
       "        (x_upd): Upsample()\n",
       "        (embed_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=512, out_features=256, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Identity()\n",
       "      )\n",
       "    )\n",
       "    (9): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 512, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (embed_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=512, out_features=256, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (10): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 512, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (embed_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=512, out_features=256, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (11): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 384, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (embed_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=512, out_features=256, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (1): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Upsample()\n",
       "        (x_upd): Upsample()\n",
       "        (embed_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=512, out_features=256, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Identity()\n",
       "      )\n",
       "    )\n",
       "    (12): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 384, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(384, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (embed_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=512, out_features=128, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 128, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (13): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (embed_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=512, out_features=128, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 128, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (14): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (embed_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=512, out_features=128, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 128, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (1): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 128, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Upsample()\n",
       "        (x_upd): Upsample()\n",
       "        (embed_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=512, out_features=128, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 128, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Identity()\n",
       "      )\n",
       "    )\n",
       "    (15): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (embed_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=512, out_features=128, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 128, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (16): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (embed_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=512, out_features=128, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 128, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (17): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (embed_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=512, out_features=128, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 128, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (out): Sequential(\n",
       "    (0): GroupNorm32(32, 128, eps=1e-05, affine=True)\n",
       "    (1): SiLU()\n",
       "    (2): Conv2d(128, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c2bd52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6452bcab",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = torch.rand(10, 3, 256, 256)\n",
    "t_batch = torch.tensor([1], device=x_test.device).repeat(x_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "146027e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unet.middle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "04f947af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 512, 8, 8])\n",
      "torch.Size([10, 512, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "y_test = unet(x_test, t_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a754a6d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 512, 8, 8])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y_test[-1]-y_test[-2]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "beb46c26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l=[1, 2, 3, 4, 5, 6, 8]\n",
    "l.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41022d2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ed570fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "TE = unet.time_embedding(t_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e11cb82b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 512])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TE.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bbb703",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f7374127",
   "metadata": {},
   "outputs": [],
   "source": [
    "down = unet.down()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0d811090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 512])\n",
      "shape h0 torch.Size([10, 3, 224, 224])\n",
      "shape h1 torch.Size([10, 128, 224, 224])\n",
      "shape h2 torch.Size([10, 128, 224, 224])\n",
      "shape h3 torch.Size([10, 128, 224, 224])\n",
      "shape h4 torch.Size([10, 128, 112, 112])\n",
      "shape h5 torch.Size([10, 128, 112, 112])\n",
      "shape h6 torch.Size([10, 128, 112, 112])\n",
      "shape h7 torch.Size([10, 128, 56, 56])\n",
      "shape h8 torch.Size([10, 256, 56, 56])\n",
      "shape h9 torch.Size([10, 256, 56, 56])\n",
      "shape h10 torch.Size([10, 256, 28, 28])\n",
      "shape h11 torch.Size([10, 256, 28, 28])\n",
      "shape h12 torch.Size([10, 256, 28, 28])\n",
      "shape h13 torch.Size([10, 256, 14, 14])\n",
      "shape h14 torch.Size([10, 512, 14, 14])\n",
      "shape h15 torch.Size([10, 512, 14, 14])\n",
      "shape h16 torch.Size([10, 512, 7, 7])\n",
      "shape h17 torch.Size([10, 512, 7, 7])\n",
      "shape h18 torch.Size([10, 512, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "time_embed = unet.time_embedding(t_batch)\n",
    "print(time_embed.shape)\n",
    "skips = []\n",
    "h = x_test.type(unet.dtype)\n",
    "print(\"shape h0\", h.shape)\n",
    "for i, module in enumerate(unet.down):\n",
    "    h = module(h, time_embed)\n",
    "    print(f\"shape h{i+1}\", h.shape)\n",
    "    skips.append(h)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9bee51c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 128, 224, 224])\n",
      "torch.Size([10, 128, 224, 224])\n",
      "torch.Size([10, 128, 224, 224])\n",
      "torch.Size([10, 128, 112, 112])\n",
      "torch.Size([10, 128, 112, 112])\n",
      "torch.Size([10, 128, 112, 112])\n",
      "torch.Size([10, 128, 56, 56])\n",
      "torch.Size([10, 256, 56, 56])\n",
      "torch.Size([10, 256, 56, 56])\n",
      "torch.Size([10, 256, 28, 28])\n",
      "torch.Size([10, 256, 28, 28])\n",
      "torch.Size([10, 256, 28, 28])\n",
      "torch.Size([10, 256, 14, 14])\n",
      "torch.Size([10, 512, 14, 14])\n",
      "torch.Size([10, 512, 14, 14])\n",
      "torch.Size([10, 512, 7, 7])\n",
      "torch.Size([10, 512, 7, 7])\n",
      "torch.Size([10, 512, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "for i in skips:\n",
    "    print(i.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "feca8c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 512, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "h = unet.middle(h, time_embed)\n",
    "print(h.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d1465919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape h0 torch.Size([10, 1024, 7, 7])\n",
      "shape H0 torch.Size([10, 512, 7, 7])\n",
      "shape h1 torch.Size([10, 1024, 7, 7])\n",
      "shape H1 torch.Size([10, 512, 7, 7])\n",
      "shape h2 torch.Size([10, 1024, 7, 7])\n",
      "shape H2 torch.Size([10, 512, 14, 14])\n",
      "shape h3 torch.Size([10, 1024, 14, 14])\n",
      "shape H3 torch.Size([10, 512, 14, 14])\n",
      "shape h4 torch.Size([10, 1024, 14, 14])\n",
      "shape H4 torch.Size([10, 512, 14, 14])\n",
      "shape h5 torch.Size([10, 768, 14, 14])\n",
      "shape H5 torch.Size([10, 512, 28, 28])\n",
      "shape h6 torch.Size([10, 768, 28, 28])\n",
      "shape H6 torch.Size([10, 256, 28, 28])\n",
      "shape h7 torch.Size([10, 512, 28, 28])\n",
      "shape H7 torch.Size([10, 256, 28, 28])\n",
      "shape h8 torch.Size([10, 512, 28, 28])\n",
      "shape H8 torch.Size([10, 256, 56, 56])\n",
      "shape h9 torch.Size([10, 512, 56, 56])\n",
      "shape H9 torch.Size([10, 256, 56, 56])\n",
      "shape h10 torch.Size([10, 512, 56, 56])\n",
      "shape H10 torch.Size([10, 256, 56, 56])\n",
      "shape h11 torch.Size([10, 384, 56, 56])\n",
      "shape H11 torch.Size([10, 256, 112, 112])\n",
      "shape h12 torch.Size([10, 384, 112, 112])\n",
      "shape H12 torch.Size([10, 128, 112, 112])\n",
      "shape h13 torch.Size([10, 256, 112, 112])\n",
      "shape H13 torch.Size([10, 128, 112, 112])\n",
      "shape h14 torch.Size([10, 256, 112, 112])\n",
      "shape H14 torch.Size([10, 128, 224, 224])\n",
      "shape h15 torch.Size([10, 256, 224, 224])\n",
      "shape H15 torch.Size([10, 128, 224, 224])\n",
      "shape h16 torch.Size([10, 256, 224, 224])\n",
      "shape H16 torch.Size([10, 128, 224, 224])\n",
      "shape h17 torch.Size([10, 256, 224, 224])\n",
      "shape H17 torch.Size([10, 128, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "for i, module in enumerate(unet.up):\n",
    "    h = torch.cat([h, skips.pop()], dim=1)\n",
    "    print(f\"shape h{i}\", h.shape)\n",
    "    h = module(h, time_embed)\n",
    "    print(f\"shape H{i}\", h.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "09672cf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 128, 224, 224])\n",
      "torch.Size([10, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "h = h.type(x_test.dtype)\n",
    "print(h.shape)\n",
    "h = unet.out(h)\n",
    "print(h.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fcb34be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = unet(x_test, t_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8849e672",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 3, 224, 224])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4913b121",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "817f9fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unet.down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e6c7f970",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0): TimestepEmbedSequential(\n",
       "    (0): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (1): TimestepEmbedSequential(\n",
       "    (0): ResBlock(\n",
       "      (in_layers): Sequential(\n",
       "        (0): GroupNorm32(32, 128, eps=1e-05, affine=True)\n",
       "        (1): SiLU()\n",
       "        (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (h_upd): Identity()\n",
       "      (x_upd): Identity()\n",
       "      (embed_layers): Sequential(\n",
       "        (0): SiLU()\n",
       "        (1): Linear(in_features=512, out_features=128, bias=True)\n",
       "      )\n",
       "      (out_layers): Sequential(\n",
       "        (0): GroupNorm32(32, 128, eps=1e-05, affine=True)\n",
       "        (1): SiLU()\n",
       "        (2): Dropout(p=0, inplace=False)\n",
       "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (skip_connection): Identity()\n",
       "    )\n",
       "  )\n",
       "  (2): TimestepEmbedSequential(\n",
       "    (0): ResBlock(\n",
       "      (in_layers): Sequential(\n",
       "        (0): GroupNorm32(32, 128, eps=1e-05, affine=True)\n",
       "        (1): SiLU()\n",
       "        (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (h_upd): Identity()\n",
       "      (x_upd): Identity()\n",
       "      (embed_layers): Sequential(\n",
       "        (0): SiLU()\n",
       "        (1): Linear(in_features=512, out_features=128, bias=True)\n",
       "      )\n",
       "      (out_layers): Sequential(\n",
       "        (0): GroupNorm32(32, 128, eps=1e-05, affine=True)\n",
       "        (1): SiLU()\n",
       "        (2): Dropout(p=0, inplace=False)\n",
       "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (skip_connection): Identity()\n",
       "    )\n",
       "  )\n",
       "  (3): TimestepEmbedSequential(\n",
       "    (0): ResBlock(\n",
       "      (in_layers): Sequential(\n",
       "        (0): GroupNorm32(32, 128, eps=1e-05, affine=True)\n",
       "        (1): SiLU()\n",
       "        (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (h_upd): Downsample(\n",
       "        (downsample): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "      )\n",
       "      (x_upd): Downsample(\n",
       "        (downsample): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "      )\n",
       "      (embed_layers): Sequential(\n",
       "        (0): SiLU()\n",
       "        (1): Linear(in_features=512, out_features=128, bias=True)\n",
       "      )\n",
       "      (out_layers): Sequential(\n",
       "        (0): GroupNorm32(32, 128, eps=1e-05, affine=True)\n",
       "        (1): SiLU()\n",
       "        (2): Dropout(p=0, inplace=False)\n",
       "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (skip_connection): Identity()\n",
       "    )\n",
       "  )\n",
       "  (4): TimestepEmbedSequential(\n",
       "    (0): ResBlock(\n",
       "      (in_layers): Sequential(\n",
       "        (0): GroupNorm32(32, 128, eps=1e-05, affine=True)\n",
       "        (1): SiLU()\n",
       "        (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (h_upd): Identity()\n",
       "      (x_upd): Identity()\n",
       "      (embed_layers): Sequential(\n",
       "        (0): SiLU()\n",
       "        (1): Linear(in_features=512, out_features=128, bias=True)\n",
       "      )\n",
       "      (out_layers): Sequential(\n",
       "        (0): GroupNorm32(32, 128, eps=1e-05, affine=True)\n",
       "        (1): SiLU()\n",
       "        (2): Dropout(p=0, inplace=False)\n",
       "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (skip_connection): Identity()\n",
       "    )\n",
       "  )\n",
       "  (5): TimestepEmbedSequential(\n",
       "    (0): ResBlock(\n",
       "      (in_layers): Sequential(\n",
       "        (0): GroupNorm32(32, 128, eps=1e-05, affine=True)\n",
       "        (1): SiLU()\n",
       "        (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (h_upd): Identity()\n",
       "      (x_upd): Identity()\n",
       "      (embed_layers): Sequential(\n",
       "        (0): SiLU()\n",
       "        (1): Linear(in_features=512, out_features=128, bias=True)\n",
       "      )\n",
       "      (out_layers): Sequential(\n",
       "        (0): GroupNorm32(32, 128, eps=1e-05, affine=True)\n",
       "        (1): SiLU()\n",
       "        (2): Dropout(p=0, inplace=False)\n",
       "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (skip_connection): Identity()\n",
       "    )\n",
       "  )\n",
       "  (6): TimestepEmbedSequential(\n",
       "    (0): ResBlock(\n",
       "      (in_layers): Sequential(\n",
       "        (0): GroupNorm32(32, 128, eps=1e-05, affine=True)\n",
       "        (1): SiLU()\n",
       "        (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (h_upd): Downsample(\n",
       "        (downsample): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "      )\n",
       "      (x_upd): Downsample(\n",
       "        (downsample): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "      )\n",
       "      (embed_layers): Sequential(\n",
       "        (0): SiLU()\n",
       "        (1): Linear(in_features=512, out_features=128, bias=True)\n",
       "      )\n",
       "      (out_layers): Sequential(\n",
       "        (0): GroupNorm32(32, 128, eps=1e-05, affine=True)\n",
       "        (1): SiLU()\n",
       "        (2): Dropout(p=0, inplace=False)\n",
       "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (skip_connection): Identity()\n",
       "    )\n",
       "  )\n",
       "  (7): TimestepEmbedSequential(\n",
       "    (0): ResBlock(\n",
       "      (in_layers): Sequential(\n",
       "        (0): GroupNorm32(32, 128, eps=1e-05, affine=True)\n",
       "        (1): SiLU()\n",
       "        (2): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (h_upd): Identity()\n",
       "      (x_upd): Identity()\n",
       "      (embed_layers): Sequential(\n",
       "        (0): SiLU()\n",
       "        (1): Linear(in_features=512, out_features=256, bias=True)\n",
       "      )\n",
       "      (out_layers): Sequential(\n",
       "        (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
       "        (1): SiLU()\n",
       "        (2): Dropout(p=0, inplace=False)\n",
       "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (skip_connection): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (8): TimestepEmbedSequential(\n",
       "    (0): ResBlock(\n",
       "      (in_layers): Sequential(\n",
       "        (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
       "        (1): SiLU()\n",
       "        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (h_upd): Identity()\n",
       "      (x_upd): Identity()\n",
       "      (embed_layers): Sequential(\n",
       "        (0): SiLU()\n",
       "        (1): Linear(in_features=512, out_features=256, bias=True)\n",
       "      )\n",
       "      (out_layers): Sequential(\n",
       "        (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
       "        (1): SiLU()\n",
       "        (2): Dropout(p=0, inplace=False)\n",
       "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (skip_connection): Identity()\n",
       "    )\n",
       "  )\n",
       "  (9): TimestepEmbedSequential(\n",
       "    (0): ResBlock(\n",
       "      (in_layers): Sequential(\n",
       "        (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
       "        (1): SiLU()\n",
       "        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (h_upd): Downsample(\n",
       "        (downsample): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "      )\n",
       "      (x_upd): Downsample(\n",
       "        (downsample): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "      )\n",
       "      (embed_layers): Sequential(\n",
       "        (0): SiLU()\n",
       "        (1): Linear(in_features=512, out_features=256, bias=True)\n",
       "      )\n",
       "      (out_layers): Sequential(\n",
       "        (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
       "        (1): SiLU()\n",
       "        (2): Dropout(p=0, inplace=False)\n",
       "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (skip_connection): Identity()\n",
       "    )\n",
       "  )\n",
       "  (10): TimestepEmbedSequential(\n",
       "    (0): ResBlock(\n",
       "      (in_layers): Sequential(\n",
       "        (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
       "        (1): SiLU()\n",
       "        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (h_upd): Identity()\n",
       "      (x_upd): Identity()\n",
       "      (embed_layers): Sequential(\n",
       "        (0): SiLU()\n",
       "        (1): Linear(in_features=512, out_features=256, bias=True)\n",
       "      )\n",
       "      (out_layers): Sequential(\n",
       "        (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
       "        (1): SiLU()\n",
       "        (2): Dropout(p=0, inplace=False)\n",
       "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (skip_connection): Identity()\n",
       "    )\n",
       "    (1): AttentionBlock(\n",
       "      (norm): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
       "      (to_qkv): Conv1d(256, 768, kernel_size=(1,), stride=(1,))\n",
       "      (attention): QKVAttention()\n",
       "      (proj_out): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "    )\n",
       "  )\n",
       "  (11): TimestepEmbedSequential(\n",
       "    (0): ResBlock(\n",
       "      (in_layers): Sequential(\n",
       "        (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
       "        (1): SiLU()\n",
       "        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (h_upd): Identity()\n",
       "      (x_upd): Identity()\n",
       "      (embed_layers): Sequential(\n",
       "        (0): SiLU()\n",
       "        (1): Linear(in_features=512, out_features=256, bias=True)\n",
       "      )\n",
       "      (out_layers): Sequential(\n",
       "        (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
       "        (1): SiLU()\n",
       "        (2): Dropout(p=0, inplace=False)\n",
       "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (skip_connection): Identity()\n",
       "    )\n",
       "    (1): AttentionBlock(\n",
       "      (norm): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
       "      (to_qkv): Conv1d(256, 768, kernel_size=(1,), stride=(1,))\n",
       "      (attention): QKVAttention()\n",
       "      (proj_out): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "    )\n",
       "  )\n",
       "  (12): TimestepEmbedSequential(\n",
       "    (0): ResBlock(\n",
       "      (in_layers): Sequential(\n",
       "        (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
       "        (1): SiLU()\n",
       "        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (h_upd): Downsample(\n",
       "        (downsample): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "      )\n",
       "      (x_upd): Downsample(\n",
       "        (downsample): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "      )\n",
       "      (embed_layers): Sequential(\n",
       "        (0): SiLU()\n",
       "        (1): Linear(in_features=512, out_features=256, bias=True)\n",
       "      )\n",
       "      (out_layers): Sequential(\n",
       "        (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
       "        (1): SiLU()\n",
       "        (2): Dropout(p=0, inplace=False)\n",
       "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (skip_connection): Identity()\n",
       "    )\n",
       "  )\n",
       "  (13): TimestepEmbedSequential(\n",
       "    (0): ResBlock(\n",
       "      (in_layers): Sequential(\n",
       "        (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
       "        (1): SiLU()\n",
       "        (2): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (h_upd): Identity()\n",
       "      (x_upd): Identity()\n",
       "      (embed_layers): Sequential(\n",
       "        (0): SiLU()\n",
       "        (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (out_layers): Sequential(\n",
       "        (0): GroupNorm32(32, 512, eps=1e-05, affine=True)\n",
       "        (1): SiLU()\n",
       "        (2): Dropout(p=0, inplace=False)\n",
       "        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (skip_connection): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (1): AttentionBlock(\n",
       "      (norm): GroupNorm32(32, 512, eps=1e-05, affine=True)\n",
       "      (to_qkv): Conv1d(512, 1536, kernel_size=(1,), stride=(1,))\n",
       "      (attention): QKVAttention()\n",
       "      (proj_out): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "    )\n",
       "  )\n",
       "  (14): TimestepEmbedSequential(\n",
       "    (0): ResBlock(\n",
       "      (in_layers): Sequential(\n",
       "        (0): GroupNorm32(32, 512, eps=1e-05, affine=True)\n",
       "        (1): SiLU()\n",
       "        (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (h_upd): Identity()\n",
       "      (x_upd): Identity()\n",
       "      (embed_layers): Sequential(\n",
       "        (0): SiLU()\n",
       "        (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (out_layers): Sequential(\n",
       "        (0): GroupNorm32(32, 512, eps=1e-05, affine=True)\n",
       "        (1): SiLU()\n",
       "        (2): Dropout(p=0, inplace=False)\n",
       "        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (skip_connection): Identity()\n",
       "    )\n",
       "    (1): AttentionBlock(\n",
       "      (norm): GroupNorm32(32, 512, eps=1e-05, affine=True)\n",
       "      (to_qkv): Conv1d(512, 1536, kernel_size=(1,), stride=(1,))\n",
       "      (attention): QKVAttention()\n",
       "      (proj_out): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "    )\n",
       "  )\n",
       "  (15): TimestepEmbedSequential(\n",
       "    (0): ResBlock(\n",
       "      (in_layers): Sequential(\n",
       "        (0): GroupNorm32(32, 512, eps=1e-05, affine=True)\n",
       "        (1): SiLU()\n",
       "        (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (h_upd): Downsample(\n",
       "        (downsample): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "      )\n",
       "      (x_upd): Downsample(\n",
       "        (downsample): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "      )\n",
       "      (embed_layers): Sequential(\n",
       "        (0): SiLU()\n",
       "        (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (out_layers): Sequential(\n",
       "        (0): GroupNorm32(32, 512, eps=1e-05, affine=True)\n",
       "        (1): SiLU()\n",
       "        (2): Dropout(p=0, inplace=False)\n",
       "        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (skip_connection): Identity()\n",
       "    )\n",
       "  )\n",
       "  (16): TimestepEmbedSequential(\n",
       "    (0): ResBlock(\n",
       "      (in_layers): Sequential(\n",
       "        (0): GroupNorm32(32, 512, eps=1e-05, affine=True)\n",
       "        (1): SiLU()\n",
       "        (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (h_upd): Identity()\n",
       "      (x_upd): Identity()\n",
       "      (embed_layers): Sequential(\n",
       "        (0): SiLU()\n",
       "        (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (out_layers): Sequential(\n",
       "        (0): GroupNorm32(32, 512, eps=1e-05, affine=True)\n",
       "        (1): SiLU()\n",
       "        (2): Dropout(p=0, inplace=False)\n",
       "        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (skip_connection): Identity()\n",
       "    )\n",
       "    (1): AttentionBlock(\n",
       "      (norm): GroupNorm32(32, 512, eps=1e-05, affine=True)\n",
       "      (to_qkv): Conv1d(512, 1536, kernel_size=(1,), stride=(1,))\n",
       "      (attention): QKVAttention()\n",
       "      (proj_out): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "    )\n",
       "  )\n",
       "  (17): TimestepEmbedSequential(\n",
       "    (0): ResBlock(\n",
       "      (in_layers): Sequential(\n",
       "        (0): GroupNorm32(32, 512, eps=1e-05, affine=True)\n",
       "        (1): SiLU()\n",
       "        (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (h_upd): Identity()\n",
       "      (x_upd): Identity()\n",
       "      (embed_layers): Sequential(\n",
       "        (0): SiLU()\n",
       "        (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (out_layers): Sequential(\n",
       "        (0): GroupNorm32(32, 512, eps=1e-05, affine=True)\n",
       "        (1): SiLU()\n",
       "        (2): Dropout(p=0, inplace=False)\n",
       "        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (skip_connection): Identity()\n",
       "    )\n",
       "    (1): AttentionBlock(\n",
       "      (norm): GroupNorm32(32, 512, eps=1e-05, affine=True)\n",
       "      (to_qkv): Conv1d(512, 1536, kernel_size=(1,), stride=(1,))\n",
       "      (attention): QKVAttention()\n",
       "      (proj_out): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unet.down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5f4ed810",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimestepEmbedSequential(\n",
       "  (0): ResBlock(\n",
       "    (in_layers): Sequential(\n",
       "      (0): GroupNorm32(32, 512, eps=1e-05, affine=True)\n",
       "      (1): SiLU()\n",
       "      (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (h_upd): Identity()\n",
       "    (x_upd): Identity()\n",
       "    (embed_layers): Sequential(\n",
       "      (0): SiLU()\n",
       "      (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "    )\n",
       "    (out_layers): Sequential(\n",
       "      (0): GroupNorm32(32, 512, eps=1e-05, affine=True)\n",
       "      (1): SiLU()\n",
       "      (2): Dropout(p=0, inplace=False)\n",
       "      (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (skip_connection): Identity()\n",
       "  )\n",
       "  (1): AttentionBlock(\n",
       "    (norm): GroupNorm32(32, 512, eps=1e-05, affine=True)\n",
       "    (to_qkv): Conv1d(512, 1536, kernel_size=(1,), stride=(1,))\n",
       "    (attention): QKVAttention()\n",
       "    (proj_out): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "  )\n",
       "  (2): ResBlock(\n",
       "    (in_layers): Sequential(\n",
       "      (0): GroupNorm32(32, 512, eps=1e-05, affine=True)\n",
       "      (1): SiLU()\n",
       "      (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (h_upd): Identity()\n",
       "    (x_upd): Identity()\n",
       "    (embed_layers): Sequential(\n",
       "      (0): SiLU()\n",
       "      (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "    )\n",
       "    (out_layers): Sequential(\n",
       "      (0): GroupNorm32(32, 512, eps=1e-05, affine=True)\n",
       "      (1): SiLU()\n",
       "      (2): Dropout(p=0, inplace=False)\n",
       "      (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (skip_connection): Identity()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unet.middle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d0a090db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): GroupNorm32(32, 128, eps=1e-05, affine=True)\n",
       "  (1): SiLU()\n",
       "  (2): Conv2d(128, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unet.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28358bff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ef986a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ec4154",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def forward(self, x, time):\n",
    "\n",
    "        time_embed = self.time_embedding(time)\n",
    "\n",
    "        skips = []\n",
    "\n",
    "        h = x.type(self.dtype)\n",
    "        for i, module in enumerate(self.down):\n",
    "            h = module(h, time_embed)\n",
    "            skips.append(h)\n",
    "        h = self.middle(h, time_embed)\n",
    "        for i, module in enumerate(self.up):\n",
    "            h = torch.cat([h, skips.pop()], dim=1)\n",
    "            h = module(h, time_embed)\n",
    "        h = h.type(x.dtype)\n",
    "        h = self.out(h)\n",
    "        return h"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
