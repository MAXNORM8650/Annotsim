{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30a6af07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import logging\n",
    "from functools import partial\n",
    "from collections import OrderedDict\n",
    "import einops\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD, IMAGENET_INCEPTION_MEAN, IMAGENET_INCEPTION_STD\n",
    "from timm.models.helpers import build_model_with_cfg, named_apply, adapt_input_conv\n",
    "from timm.models.layers import trunc_normal_, lecun_normal_, to_2tuple\n",
    "from timm.models.registry import register_model\n",
    "import timm\n",
    "\n",
    "# from helpers import complement_idx\n",
    "from einops import rearrange, reduce, repeat\n",
    "from einops.layers.torch import Rearrange, Reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa27df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Downsample(nn.Module):\n",
    "    def __init__(self, in_channels, use_conv, out_channels=None):\n",
    "        super().__init__()\n",
    "        self.channels = in_channels\n",
    "        out_channels = out_channels or in_channels\n",
    "        if use_conv:\n",
    "            # downsamples by 1/2\n",
    "            self.downsample = nn.Conv2d(in_channels, out_channels, 3, stride=2, padding=1, )\n",
    "        else:\n",
    "            assert in_channels == out_channels\n",
    "            self.downsample = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self, x, time_embed=None):\n",
    "        assert x.shape[1] == self.channels\n",
    "        return self.downsample(x)\n",
    "\n",
    "\n",
    "class Upsample(nn.Module):\n",
    "    def __init__(self, in_channels, use_conv, out_channels=None):\n",
    "        super().__init__()\n",
    "        self.channels = in_channels\n",
    "        self.use_conv = use_conv\n",
    "        # uses upsample then conv to avoid checkerboard artifacts\n",
    "        # self.upsample = nn.Upsample(scale_factor=2, mode=\"nearest\")\n",
    "        if use_conv:\n",
    "            self.conv = nn.Conv2d(in_channels, out_channels, 3, padding=1)\n",
    "\n",
    "    def forward(self, x, time_embed=None):\n",
    "        assert x.shape[1] == self.channels\n",
    "        x = F.interpolate(x, scale_factor=2, mode=\"nearest\")\n",
    "        if self.use_conv:\n",
    "            x = self.conv(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5dd59f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DAFF(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.,\n",
    "                 kernel_size=3, with_bn=True):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        # pointwise\n",
    "        self.conv1 = nn.Conv2d(in_features, hidden_features, kernel_size=1, stride=1, padding=0)\n",
    "        # depthwise\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            hidden_features, hidden_features, kernel_size=kernel_size, stride=1,\n",
    "            padding=(kernel_size - 1) // 2, groups=hidden_features)\n",
    "        \n",
    "        # pointwise\n",
    "        self.conv3 = nn.Conv2d(hidden_features, out_features, kernel_size=1, stride=1, padding=0)\n",
    "        self.act = act_layer()\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm2d(hidden_features)\n",
    "        self.bn2 = nn.BatchNorm2d(hidden_features)\n",
    "        self.bn3 = nn.BatchNorm2d(out_features)\n",
    "        \n",
    "        # The reduction ratio is always set to 4\n",
    "        self.squeeze = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.compress = nn.Linear(in_features, in_features//4)\n",
    "        self.excitation = nn.Linear(in_features//4, in_features)\n",
    "                \n",
    "    def forward(self, x):\n",
    "        B, N, C = x.size()\n",
    "        cls_token, tokens = torch.split(x, [1, N - 1], dim=1)\n",
    "        print(cls_token.shape)\n",
    "        print(tokens.shape)\n",
    "        x = tokens.reshape(B, int(math.sqrt(N - 1)), int(math.sqrt(N - 1)), C).permute(0, 3, 1, 2)\n",
    "        print(x.shape)\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.act(x)\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.act(x)\n",
    "        x = shortcut + x\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "\n",
    "        weight = self.squeeze(x).flatten(1).reshape(B, 1, C)\n",
    "        weight = self.excitation(self.act(self.compress(weight)))\n",
    "        cls_token = cls_token * weight\n",
    "        \n",
    "        tokens = x.flatten(2).permute(0, 2, 1)\n",
    "        out = torch.cat((cls_token, tokens), dim=1)\n",
    "        \n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "38087f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim= 768\n",
    "x_test = torch.rand(1, 197, 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a388e591",
   "metadata": {},
   "outputs": [],
   "source": [
    "daff = DAFF(in_features=dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7fa4b393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 768])\n",
      "torch.Size([1, 196, 768])\n",
      "torch.Size([1, 768, 14, 14])\n"
     ]
    }
   ],
   "source": [
    "y_test = daff(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5794027c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 197, 768])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b72d0a1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 768])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (3072x256 and 65536x256)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 44\u001b[0m\n\u001b[0;32m     42\u001b[0m input_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m768\u001b[39m)\n\u001b[0;32m     43\u001b[0m model \u001b[38;5;241m=\u001b[39m MyNetwork()\n\u001b[1;32m---> 44\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(output\u001b[38;5;241m.\u001b[39mshape)  \u001b[38;5;66;03m# Should print torch.Size([4, 256, 256])\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\env22\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[46], line 34\u001b[0m, in \u001b[0;36mMyNetwork.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Reshape patches to match the embedding dimension\u001b[39;00m\n\u001b[0;32m     33\u001b[0m patches \u001b[38;5;241m=\u001b[39m patches\u001b[38;5;241m.\u001b[39mview(batch_size, num_patches, embedding_dim)\n\u001b[1;32m---> 34\u001b[0m patches \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpatch_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpatches\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Reshape and apply deconvolutions\u001b[39;00m\n\u001b[0;32m     37\u001b[0m patches \u001b[38;5;241m=\u001b[39m patches\u001b[38;5;241m.\u001b[39mview(batch_size, \u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m16\u001b[39m, \u001b[38;5;241m16\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\env22\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\env22\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (3072x256 and 65536x256)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define a custom convolutional layer with 3x3 kernel\n",
    "def conv3x3(in_channels, out_channels):\n",
    "    return nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "\n",
    "class MyNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyNetwork, self).__init__()\n",
    "        \n",
    "        # Define the layers for converting to patches\n",
    "        self.patch_extraction = nn.Unfold(kernel_size=16, stride=16)  # Extract patches of size (16, 16)\n",
    "        self.patch_embedding = nn.Linear(16 * 16 * 256, 256)  # Embed patches into the desired dimension\n",
    "\n",
    "        # Define the layers for upsampling back to (256, 256)\n",
    "        self.upsample = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),  # Double spatial dimensions\n",
    "            nn.GELU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),  # Double spatial dimensions\n",
    "            nn.GELU(),\n",
    "            nn.ConvTranspose2d(64, 256, kernel_size=4, stride=2, padding=1),  # Double spatial dimensions\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Extract patches and reshape\n",
    "        patches = self.patch_extraction(x)\n",
    "        print(patches.shape)\n",
    "        # Get the number of patches and the embedding dimension\n",
    "        num_patches, embedding_dim = patches.size(1), 256\n",
    "        batch_size = x.size(0)\n",
    "        # Reshape patches to match the embedding dimension\n",
    "        patches = patches.view(batch_size, num_patches, embedding_dim)\n",
    "        patches = self.patch_embedding(patches)\n",
    "\n",
    "        # Reshape and apply deconvolutions\n",
    "        patches = patches.view(batch_size, 256, 16, 16)\n",
    "        x = self.upsample(patches)\n",
    "        return x\n",
    "\n",
    "# Example usage with batch size 4:\n",
    "input_tensor = torch.randn(4, 256, 768)\n",
    "model = MyNetwork()\n",
    "output = model(input_tensor)\n",
    "print(output.shape)  # Should print torch.Size([4, 256, 256])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45e47c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
