{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c79ed566",
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import torch\n",
    "import cv2\n",
    "import h5py\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import os \n",
    "import pandas as pd\n",
    "from torchvision import datasets, transforms\n",
    "from PIL import Image\n",
    "import utils\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b4827603",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = './'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "20814c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in open(os.path.join(root,'5k_Healthy.txt')):\n",
    "#     print(i.split(\"\\\\\")[-1][:-5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "b7a5935d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HnABRATS(torch.utils.data.Dataset):\n",
    "    def __init__(self, ROOT_DIR, img_size = (256, 256), is_train = True, transform = None, num_mod = 1):     \n",
    "        self.D = num_mod\n",
    "        self.transform = transforms.Compose(\n",
    "            [transforms.ToPILImage(),        \n",
    "             transforms.Resize(img_size, transforms.InterpolationMode.BILINEAR),\n",
    "             # transforms.CenterCrop(256),\n",
    "             transforms.ToTensor(),\n",
    "             transforms.Normalize((0.5), (0.5))\n",
    "            ]\n",
    "        ) if not transform else transform\n",
    "\n",
    "        self.root = os.path.join(ROOT_DIR, \"DATASETS/brats2021\")\n",
    "        if is_train:\n",
    "            self.filenames = [i for i in open(os.path.join(self.root,'Healthy.txt'))]\n",
    "        else:\n",
    "            self.filenames = [i for i in open(os.path.join(self.root,'Anomaly.txt'))]\n",
    "            self.mask_filenames = [i for i in open(os.path.join(self.root,'Mask.txt'))]\n",
    "        self.img_size = img_size\n",
    "        self.is_train = is_train\n",
    "        if \".DS_Store\" in self.filenames:\n",
    "            self.filenames.remove(\".DS_Store\")\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        image = np.load(self.filenames[idx][:-1])\n",
    "#         print(image)\n",
    "        if self.D == 4:\n",
    "            if self.transform:\n",
    "                for i in range(image.shape[0]):\n",
    "                    image1 = self.transform(image[0])\n",
    "                    image2 = self.transform(image[1])\n",
    "                    image3 = self.transform(image[2])\n",
    "                    image4 = self.transform(image[3])\n",
    "                image = torch.stack([image1, image2, image3, image4], dim = 1)\n",
    "        else:\n",
    "            image = self.transform(image)\n",
    "        if not self.is_train:\n",
    "            mask = np.load(self.mask_filenames[idx][:-1])\n",
    "            sample = {'image': image, \"filenames\": self.filenames[idx].split(\"/\")[-1][:-4], \"mask\":mask}\n",
    "        else:\n",
    "            sample = {'image': image, \"filenames\": self.filenames[idx].split(\"/\")[-1][:-4]}\n",
    "        return sample\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "4cf3e0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnnoDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, ROOT_DIR, set = \"train\", input_size = 256, channels = 1):\n",
    "        super().__init__()\n",
    "        self.root = os.path.join(ROOT_DIR, \"DATASETS/brainMRI\")\n",
    "        self.se = set\n",
    "        self.input_size = input_size\n",
    "        self.get_directory_paths()\n",
    "        self.channels = channels\n",
    "    def __getitem__(self, idx):\n",
    "        if self.se == \"train\":\n",
    "            if self.channels==3:\n",
    "                img = Image.open(self.file_paths[idx]).convert('RGB')\n",
    "            else:\n",
    "                img = Image.open(self.file_paths[idx]).convert('L')\n",
    "            img = np.array(img)\n",
    "            filenames = self.file_names[idx]\n",
    "#             if len(img.shape) == 2:\n",
    "#                 img = np.stack([img] * 3, 2)\n",
    "            img = Image.fromarray(img)\n",
    "            img = transforms.Resize(self.input_size)(img)\n",
    "#             img = transforms.RandomResizedCrop(size=self.input_size,scale=(0.4, 0.75),ratio=(0.5,1.5))(img)\n",
    "#             img = transforms.RandomCrop(self.input_size)(img)\n",
    "            img = transforms.RandomHorizontalFlip()(img)\n",
    "            img = transforms.ColorJitter(brightness=0.2, contrast=0.2)(img)\n",
    "            img = transforms.ToTensor()(img)\n",
    "#             img = transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])(img)\n",
    "            mask = torch.zeros(self.input_size)\n",
    "        elif self.se == 'test':\n",
    "            if self.file_names[idx] in self.mfile_names:\n",
    "                index = self.mfile_names.index(self.file_names[idx])\n",
    "                mask = Image.open(self.mfile_paths[index])\n",
    "                mask = np.array(mask)\n",
    "                if self.channels==3:\n",
    "                    mask = np.stack((mask,) * self.channels, axis=-1)\n",
    "                mask = Image.fromarray(mask)             \n",
    "                mask = transforms.Resize(self.input_size)(mask)\n",
    "                mask = transforms.ToTensor()(mask)\n",
    "                mask = torch.where(mask>0, 1.0, 0.0)\n",
    "#                 print(\"mask1\", mask.shape)\n",
    "            else:\n",
    "                mask = torch.zeros(self.channels, *self.input_size)\n",
    "#                 print(\"mask2\", mask.shape)\n",
    "            if self.channels==3:\n",
    "                img = Image.open(self.file_paths[idx]).convert('RGB')\n",
    "            else:\n",
    "                img = Image.open(self.file_paths[idx]).convert('L')\n",
    "#             img = Image.open(self.file_paths[idx]).convert('L')\n",
    "            img = np.array(img)\n",
    "            filenames = self.file_names[idx]\n",
    "#             if len(img.shape) == 2:\n",
    "#                 img = np.stack([img] * 3, 2)\n",
    "            img = Image.fromarray(img)\n",
    "            img = transforms.Resize(self.input_size)(img)\n",
    "#             img = transforms.RandomResizedCrop(size=self.input_size,scale=(0.4, 0.75),ratio=(0.5,1.5))(img)\n",
    "#             img = transforms.RandomCrop(self.input_size)(img)\n",
    "#             img = transforms.RandomHorizontalFlip()(img)\n",
    "#             img = transforms.ColorJitter(brightness=0.2, contrast=0.2)(img)\n",
    "            img = transforms.ToTensor()(img)\n",
    "#             img = transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])(img)\n",
    "#             target  = None\n",
    "#         print(img.max())\n",
    "#         print(img.min())\n",
    "        return  {'image': img, \"filenames\": filenames, \"mask\": mask, \"label\" : 1 if mask.max()>0 else 0}\n",
    "    def get_directory_paths(self):\n",
    "        self.file_paths = []\n",
    "        self.file_names = []\n",
    "        if self.se == 'train':\n",
    "            root_dir = os.path.join(self.root, self.se)\n",
    "            for rout, dirs, files in os.walk(root_dir):\n",
    "                if not dirs:\n",
    "                    for f in files:\n",
    "                        self.file_paths.append(os.path.join(rout, f))\n",
    "                        self.file_names.append(f)\n",
    "        elif self.se == 'test':\n",
    "            self.mfile_paths = []\n",
    "            self.mfile_names = []\n",
    "            root_dir = os.path.join(self.root, self.se)\n",
    "            mask_dir = os.path.join(self.root, \"ground_truth\")\n",
    "            for rout, dirs, files in os.walk(root_dir):\n",
    "                if not dirs:\n",
    "                    for f in files:\n",
    "                        self.file_paths.append(os.path.join(rout, f))\n",
    "                        self.file_names.append(f)\n",
    "            for rout, dirs, files in os.walk(mask_dir):\n",
    "                if not dirs:\n",
    "                    for f in files:\n",
    "                        self.mfile_paths.append(os.path.join(rout, f))\n",
    "                        self.mfile_names.append(f)    \n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "42dedd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = HnABRATS(root, is_train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "6aaeea0e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92631"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "3d1001f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0038090038314176245"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(0.0337*11.8) /104.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "1920ec35",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = AnnoDataset(root, set='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "898ec4e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "154"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "10661f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "Average percentage of abnormal pixels: 7.31%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming dataset is your list of images and masks\n",
    "total_abnormal_pixels = 0\n",
    "total_pixels = 0\n",
    "\n",
    "for j,i in enumerate(data):\n",
    "    mask = np.float16(i[\"mask\"]>0)   #[0][0]\n",
    "    abnormal_pixel_count = np.sum(mask)\n",
    "    total_abnormal_pixels += abnormal_pixel_count\n",
    "    total_pixels += mask.size\n",
    "    print(j)\n",
    "\n",
    "# Calculate average percentage\n",
    "average_percentage_abnormal = (total_abnormal_pixels / total_pixels) * 100\n",
    "\n",
    "print(f\"Average percentage of abnormal pixels: {average_percentage_abnormal:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "7f0d03cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80400.73202614379"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_pixels/153"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "630dd6a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6433214254596151"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3.72*(5070/11477)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "0e661ff1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5837.012987012987"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_abnormal_pixels/154"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "67eeeb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"C:\\Users\\Admin\\Documents\\Anomaly Detection\\AnoDDPM\\DATASETS\\brats2021\\traning_data\\BraTS2021_00005\\BraTS2021_00005+_Healthy_+30.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "f3f5e5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = np.load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "e8ee488f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.79723364"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "818bcc25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(i[\"mask\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1dc9ed05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.float16(i[\"mask\"]>0).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a7fc21ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 256, 256])\n",
      "torch.Size([1, 4, 256, 256])\n",
      "torch.Size([1, 4, 256, 256])\n",
      "torch.Size([1, 4, 256, 256])\n",
      "torch.Size([1, 4, 256, 256])\n",
      "torch.Size([1, 4, 256, 256])\n",
      "torch.Size([1, 4, 256, 256])\n",
      "torch.Size([1, 4, 256, 256])\n",
      "torch.Size([1, 4, 256, 256])\n",
      "torch.Size([1, 4, 256, 256])\n",
      "torch.Size([1, 4, 256, 256])\n",
      "torch.Size([1, 4, 256, 256])\n",
      "torch.Size([1, 4, 256, 256])\n",
      "torch.Size([1, 4, 256, 256])\n",
      "torch.Size([1, 4, 256, 256])\n",
      "torch.Size([1, 4, 256, 256])\n",
      "torch.Size([1, 4, 256, 256])\n",
      "torch.Size([1, 4, 256, 256])\n",
      "torch.Size([1, 4, 256, 256])\n",
      "torch.Size([1, 4, 256, 256])\n",
      "torch.Size([1, 4, 256, 256])\n",
      "torch.Size([1, 4, 256, 256])\n",
      "torch.Size([1, 4, 256, 256])\n",
      "torch.Size([1, 4, 256, 256])\n",
      "torch.Size([1, 4, 256, 256])\n",
      "torch.Size([1, 4, 256, 256])\n",
      "torch.Size([1, 4, 256, 256])\n",
      "torch.Size([1, 4, 256, 256])\n",
      "torch.Size([1, 4, 256, 256])\n",
      "torch.Size([1, 4, 256, 256])\n",
      "torch.Size([1, 4, 256, 256])\n",
      "torch.Size([1, 4, 256, 256])\n",
      "torch.Size([1, 4, 256, 256])\n",
      "torch.Size([1, 4, 256, 256])\n",
      "torch.Size([1, 4, 256, 256])\n",
      "torch.Size([1, 4, 256, 256])\n",
      "torch.Size([1, 4, 256, 256])\n",
      "torch.Size([1, 4, 256, 256])\n",
      "torch.Size([1, 4, 256, 256])\n",
      "torch.Size([1, 4, 256, 256])\n",
      "torch.Size([1, 4, 256, 256])\n",
      "torch.Size([1, 4, 256, 256])\n",
      "torch.Size([1, 4, 256, 256])\n",
      "torch.Size([1, 4, 256, 256])\n",
      "torch.Size([1, 4, 256, 256])\n",
      "torch.Size([1, 4, 256, 256])\n",
      "torch.Size([1, 4, 256, 256])\n",
      "torch.Size([1, 4, 256, 256])\n",
      "torch.Size([1, 4, 256, 256])\n",
      "torch.Size([1, 4, 256, 256])\n",
      "torch.Size([1, 4, 256, 256])\n",
      "torch.Size([1, 4, 256, 256])\n",
      "torch.Size([1, 4, 256, 256])\n",
      "torch.Size([1, 4, 256, 256])\n",
      "torch.Size([1, 4, 256, 256])\n",
      "torch.Size([1, 4, 256, 256])\n",
      "torch.Size([1, 4, 256, 256])\n",
      "torch.Size([1, 4, 256, 256])\n",
      "torch.Size([1, 4, 256, 256])\n",
      "torch.Size([1, 4, 256, 256])\n",
      "torch.Size([1, 4, 256, 256])\n",
      "torch.Size([1, 4, 256, 256])\n",
      "torch.Size([1, 4, 256, 256])\n",
      "torch.Size([1, 4, 256, 256])\n",
      "torch.Size([1, 4, 256, 256])\n",
      "torch.Size([1, 4, 256, 256])\n",
      "torch.Size([1, 4, 256, 256])\n",
      "torch.Size([1, 4, 256, 256])\n",
      "torch.Size([1, 4, 256, 256])\n",
      "torch.Size([1, 4, 256, 256])\n",
      "torch.Size([1, 4, 256, 256])\n",
      "torch.Size([1, 4, 256, 256])\n",
      "torch.Size([1, 4, 256, 256])\n",
      "torch.Size([1, 4, 256, 256])\n",
      "torch.Size([1, 4, 256, 256])\n",
      "torch.Size([1, 4, 256, 256])\n",
      "torch.Size([1, 4, 256, 256])\n",
      "torch.Size([1, 4, 256, 256])\n",
      "torch.Size([1, 4, 256, 256])\n",
      "torch.Size([1, 4, 256, 256])\n",
      "torch.Size([1, 4, 256, 256])\n",
      "torch.Size([1, 4, 256, 256])\n",
      "torch.Size([1, 4, 256, 256])\n",
      "torch.Size([1, 4, 256, 256])\n",
      "torch.Size([1, 4, 256, 256])\n",
      "torch.Size([1, 4, 256, 256])\n",
      "torch.Size([1, 4, 256, 256])\n",
      "torch.Size([1, 4, 256, 256])\n",
      "torch.Size([1, 4, 256, 256])\n",
      "torch.Size([1, 4, 256, 256])\n",
      "torch.Size([1, 4, 256, 256])\n",
      "torch.Size([1, 4, 256, 256])\n",
      "torch.Size([1, 4, 256, 256])\n",
      "torch.Size([1, 4, 256, 256])\n",
      "torch.Size([1, 4, 256, 256])\n",
      "torch.Size([1, 4, 256, 256])\n",
      "torch.Size([1, 4, 256, 256])\n",
      "torch.Size([1, 4, 256, 256])\n",
      "torch.Size([1, 4, 256, 256])\n",
      "torch.Size([1, 4, 256, 256])\n",
      "torch.Size([1, 4, 256, 256])\n",
      "torch.Size([1, 4, 256, 256])\n",
      "torch.Size([1, 4, 256, 256])\n",
      "torch.Size([1, 4, 256, 256])\n",
      "torch.Size([1, 4, 256, 256])\n",
      "torch.Size([1, 4, 256, 256])\n",
      "torch.Size([1, 4, 256, 256])\n",
      "torch.Size([1, 4, 256, 256])\n",
      "torch.Size([1, 4, 256, 256])\n",
      "torch.Size([1, 4, 256, 256])\n",
      "torch.Size([1, 4, 256, 256])\n",
      "torch.Size([1, 4, 256, 256])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[61], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m Data:\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(i[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mshape)\n",
      "Cell \u001b[1;32mIn[47], line 34\u001b[0m, in \u001b[0;36mHnABRATS.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(image\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]):\n\u001b[0;32m     33\u001b[0m     image1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(image[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m---> 34\u001b[0m     image2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m     image3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(image[\u001b[38;5;241m2\u001b[39m])\n\u001b[0;32m     36\u001b[0m     image4 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(image[\u001b[38;5;241m3\u001b[39m])\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\env22\\lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\env22\\lib\\site-packages\\torchvision\\transforms\\transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[0;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\env22\\lib\\site-packages\\torchvision\\transforms\\functional.py:166\u001b[0m, in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;66;03m# handle PIL Image\u001b[39;00m\n\u001b[0;32m    165\u001b[0m mode_to_nptype \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mint32, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI;16\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mint16, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mfloat32}\n\u001b[1;32m--> 166\u001b[0m img \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode_to_nptype\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muint8\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m)\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pic\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    169\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m255\u001b[39m \u001b[38;5;241m*\u001b[39m img\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in Data:\n",
    "    print(i[\"mask\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4ed3fe90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i['mask'][200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2366438c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(240, 155)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i['mask'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c647429b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.0"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i['mask'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cd57be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
