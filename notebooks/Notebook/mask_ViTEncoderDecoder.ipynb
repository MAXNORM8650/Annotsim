{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a024474",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, List, Optional, Tuple, Union\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b7d60a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Layer/Module Helpers\n",
    "\n",
    "Hacked together by / Copyright 2020 Ross Wightman\n",
    "\"\"\"\n",
    "from itertools import repeat\n",
    "import collections.abc\n",
    "\n",
    "\n",
    "# From PyTorch internals\n",
    "def _ntuple(n):\n",
    "    def parse(x):\n",
    "        if isinstance(x, collections.abc.Iterable) and not isinstance(x, str):\n",
    "            return tuple(x)\n",
    "        return tuple(repeat(x, n))\n",
    "    return parse\n",
    "\n",
    "\n",
    "to_1tuple = _ntuple(1)\n",
    "to_2tuple = _ntuple(2)\n",
    "to_3tuple = _ntuple(3)\n",
    "to_4tuple = _ntuple(4)\n",
    "to_ntuple = _ntuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cab792a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention mode is math\n"
     ]
    }
   ],
   "source": [
    "if hasattr(torch.nn.functional, 'scaled_dot_product_attention'):\n",
    "    ATTENTION_MODE = 'flash'\n",
    "else:\n",
    "    try:\n",
    "        import xformers\n",
    "        import xformers.ops\n",
    "        ATTENTION_MODE = 'xformers'\n",
    "    except:\n",
    "        ATTENTION_MODE = 'math'\n",
    "print(f'attention mode is {ATTENTION_MODE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff48e9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from torch import _assert\n",
    "except ImportError:\n",
    "    def _assert(condition: bool, message: str):\n",
    "        assert condition, message\n",
    "\n",
    "\n",
    "def _float_to_int(x: float) -> int:\n",
    "    \"\"\"\n",
    "    Symbolic tracing helper to substitute for inbuilt `int`.\n",
    "    Hint: Inbuilt `int` can't accept an argument of type `Proxy`\n",
    "    \"\"\"\n",
    "    return int(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "980c8bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from typing import Union\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "class Format(str, Enum):\n",
    "    NCHW = 'NCHW'\n",
    "    NHWC = 'NHWC'\n",
    "    NCL = 'NCL'\n",
    "    NLC = 'NLC'\n",
    "\n",
    "\n",
    "FormatT = Union[str, Format]\n",
    "\n",
    "\n",
    "def get_spatial_dim(fmt: FormatT):\n",
    "    fmt = Format(fmt)\n",
    "    if fmt is Format.NLC:\n",
    "        dim = (1,)\n",
    "    elif fmt is Format.NCL:\n",
    "        dim = (2,)\n",
    "    elif fmt is Format.NHWC:\n",
    "        dim = (1, 2)\n",
    "    else:\n",
    "        dim = (2, 3)\n",
    "    return dim\n",
    "\n",
    "\n",
    "def get_channel_dim(fmt: FormatT):\n",
    "    fmt = Format(fmt)\n",
    "    if fmt is Format.NHWC:\n",
    "        dim = 3\n",
    "    elif fmt is Format.NLC:\n",
    "        dim = 2\n",
    "    else:\n",
    "        dim = 1\n",
    "    return dim\n",
    "\n",
    "\n",
    "def nchw_to(x: torch.Tensor, fmt: Format):\n",
    "    if fmt == Format.NHWC:\n",
    "        x = x.permute(0, 2, 3, 1)\n",
    "    elif fmt == Format.NLC:\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "    elif fmt == Format.NCL:\n",
    "        x = x.flatten(2)\n",
    "    return x\n",
    "\n",
    "\n",
    "def nhwc_to(x: torch.Tensor, fmt: Format):\n",
    "    if fmt == Format.NCHW:\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "    elif fmt == Format.NLC:\n",
    "        x = x.flatten(1, 2)\n",
    "    elif fmt == Format.NCL:\n",
    "        x = x.flatten(1, 2).transpose(1, 2)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "537349f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from functools import partial\n",
    "\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.utils.checkpoint\n",
    "# import numpy as np\n",
    "# import einops\n",
    "# import math\n",
    "# class DropPath(nn.Module):\n",
    "#     \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n",
    "#     \"\"\"\n",
    "#     def __init__(self, drop_prob=None):\n",
    "#         super(DropPath, self).__init__()\n",
    "#         self.drop_prob = drop_prob\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return drop_path(x, self.drop_prob, self.training)\n",
    "\n",
    "\n",
    "# class Mlp(nn.Module):\n",
    "#     def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "#         super().__init__()\n",
    "#         out_features = out_features or in_features\n",
    "#         hidden_features = hidden_features or in_features\n",
    "#         self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "#         self.act = act_layer()\n",
    "#         self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "#         self.drop = nn.Dropout(drop)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.fc1(x)\n",
    "#         x = self.act(x)\n",
    "#         x = self.drop(x)\n",
    "#         x = self.fc2(x)\n",
    "#         x = self.drop(x)\n",
    "#         return x\n",
    "# class Attention(nn.Module):\n",
    "#     def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
    "#         super().__init__()\n",
    "#         self.num_heads = num_heads\n",
    "#         head_dim = dim // num_heads\n",
    "#         self.scale = qk_scale or head_dim ** -0.5\n",
    "\n",
    "#         self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "#         self.attn_drop = nn.Dropout(attn_drop)\n",
    "#         self.proj = nn.Linear(dim, dim)\n",
    "#         self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         B, L, C = x.shape\n",
    "\n",
    "#         qkv = self.qkv(x)\n",
    "#         if ATTENTION_MODE == 'flash':\n",
    "#             qkv = einops.rearrange(qkv, 'B L (K H D) -> K B H L D', K=3, H=self.num_heads).float()\n",
    "#             q, k, v = qkv[0], qkv[1], qkv[2]  # B H L D\n",
    "#             x = torch.nn.functional.scaled_dot_product_attention(q, k, v)\n",
    "#             x = einops.rearrange(x, 'B H L D -> B L (H D)')\n",
    "#         elif ATTENTION_MODE == 'xformers':\n",
    "#             qkv = einops.rearrange(qkv, 'B L (K H D) -> K B L H D', K=3, H=self.num_heads)\n",
    "#             q, k, v = qkv[0], qkv[1], qkv[2]  # B L H D\n",
    "#             x = xformers.ops.memory_efficient_attention(q, k, v)\n",
    "#             x = einops.rearrange(x, 'B L H D -> B L (H D)', H=self.num_heads)\n",
    "#         elif ATTENTION_MODE == 'math':\n",
    "#             qkv = einops.rearrange(qkv, 'B L (K H D) -> K B H L D', K=3, H=self.num_heads)\n",
    "#             q, k, v = qkv[0], qkv[1], qkv[2]  # B H L D\n",
    "#             attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "#             attn = attn.softmax(dim=-1)\n",
    "#             attn = self.attn_drop(attn)\n",
    "#             x = (attn @ v).transpose(1, 2).reshape(B, L, C)\n",
    "#         else:\n",
    "#             raise NotImplemented\n",
    "\n",
    "#         x = self.proj(x)\n",
    "#         x = self.proj_drop(x)\n",
    "#         return x\n",
    "\n",
    "# class Block(nn.Module):\n",
    "\n",
    "#     def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None,\n",
    "#                  act_layer=nn.GELU, norm_layer=nn.LayerNorm, skip=False, use_checkpoint=False):\n",
    "#         super().__init__()\n",
    "#         self.norm1 = norm_layer(dim)\n",
    "#         self.attn = Attention(\n",
    "#             dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale)\n",
    "#         self.norm2 = norm_layer(dim)\n",
    "#         mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "#         self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer)\n",
    "#         self.skip_linear = nn.Linear(2 * dim, dim) if skip else None\n",
    "#         self.use_checkpoint = use_checkpoint\n",
    "\n",
    "#     def forward(self, x, skip=None):\n",
    "#         if self.use_checkpoint:\n",
    "#             return torch.utils.checkpoint.checkpoint(self._forward, x, skip)\n",
    "#         else:\n",
    "#             return self._forward(x, skip)\n",
    "\n",
    "#     def _forward(self, x, skip=None):\n",
    "#         if self.skip_linear is not None:\n",
    "#             x = self.skip_linear(torch.cat([x, skip], dim=-1))\n",
    "#         x = x + self.attn(self.norm1(x))\n",
    "#         x = x + self.mlp(self.norm2(x))\n",
    "#         return x\n",
    "\n",
    "\n",
    "# class PatchEmbed(nn.Module):\n",
    "#     \"\"\" 2D Image to Patch Embedding\n",
    "#     \"\"\"\n",
    " \n",
    "#     def __init__(\n",
    "#             self,\n",
    "#             img_size: Optional[int] = 224,\n",
    "#             patch_size: int = 16,\n",
    "#             in_chans: int = 3,\n",
    "#             embed_dim: int = 768,\n",
    "#             norm_layer: Optional[Callable] = None,\n",
    "#             flatten: bool = True,\n",
    "#             output_fmt: Optional[str] = None,\n",
    "#             bias: bool = True,\n",
    "#             strict_img_size: bool = True,\n",
    "#     ):\n",
    "#         super().__init__()\n",
    "#         self.patch_size = to_2tuple(patch_size)\n",
    "#         if img_size is not None:\n",
    "#             self.img_size = to_2tuple(img_size)\n",
    "#             self.grid_size = tuple([s // p for s, p in zip(self.img_size, self.patch_size)])\n",
    "#             self.num_patches = self.grid_size[0] * self.grid_size[1]\n",
    "#         else:\n",
    "#             self.img_size = None\n",
    "#             self.grid_size = None\n",
    "#             self.num_patches = None\n",
    "\n",
    "#         if output_fmt is not None:\n",
    "#             self.flatten = False\n",
    "#             self.output_fmt = Format(output_fmt)\n",
    "#         else:\n",
    "#             # flatten spatial dim and transpose to channels last, kept for bwd compat\n",
    "#             self.flatten = flatten\n",
    "#             self.output_fmt = Format.NCHW\n",
    "#         self.strict_img_size = strict_img_size\n",
    "\n",
    "#         self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size, bias=bias)\n",
    "#         self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity()\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         B, C, H, W = x.shape\n",
    "#         if self.img_size is not None:\n",
    "#             if self.strict_img_size:\n",
    "#                 _assert(H == self.img_size[0], f\"Input height ({H}) doesn't match model ({self.img_size[0]}).\")\n",
    "#                 _assert(W == self.img_size[1], f\"Input width ({W}) doesn't match model ({self.img_size[1]}).\")\n",
    "#             else:\n",
    "#                 _assert(\n",
    "#                     H % self.patch_size[0] == 0,\n",
    "#                     f\"Input height ({H}) should be divisible by patch size ({self.patch_size[0]}).\"\n",
    "#                 )\n",
    "#                 _assert(\n",
    "#                     W % self.patch_size[1] == 0,\n",
    "#                     f\"Input width ({W}) should be divisible by patch size ({self.patch_size[1]}).\"\n",
    "#                 )\n",
    "\n",
    "#         x = self.proj(x)\n",
    "#         if self.flatten:\n",
    "#             x = x.flatten(2).transpose(1, 2)  # NCHW -> NLC\n",
    "#         elif self.output_fmt != Format.NCHW:\n",
    "#             x = nchw_to(x, self.output_fmt)\n",
    "#         x = self.norm(x)\n",
    "#         return x\n",
    "# def get_2d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):\n",
    "#     \"\"\"\n",
    "#     grid_size: int of the grid height and width\n",
    "#     return:\n",
    "#     pos_embed: [grid_size*grid_size, embed_dim] or [1+grid_size*grid_size, embed_dim] (w/ or w/o cls_token)\n",
    "#     \"\"\"\n",
    "#     grid_h = np.arange(grid_size, dtype=np.float32)\n",
    "#     grid_w = np.arange(grid_size, dtype=np.float32)\n",
    "#     grid = np.meshgrid(grid_w, grid_h)  # here w goes first\n",
    "#     grid = np.stack(grid, axis=0)\n",
    "\n",
    "#     grid = grid.reshape([2, 1, grid_size, grid_size])\n",
    "#     pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
    "#     if cls_token:\n",
    "#         pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
    "#     return pos_embed\n",
    "\n",
    "\n",
    "# def get_2d_sincos_pos_embed_from_grid(embed_dim, grid):\n",
    "#     assert embed_dim % 2 == 0\n",
    "\n",
    "#     # use half of dimensions to encode grid_h\n",
    "#     emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])  # (H*W, D/2)\n",
    "#     emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])  # (H*W, D/2)\n",
    "\n",
    "#     emb = np.concatenate([emb_h, emb_w], axis=1) # (H*W, D)\n",
    "#     return emb\n",
    "\n",
    "\n",
    "# def get_1d_sincos_pos_embed_from_grid(embed_dim, pos):\n",
    "#     \"\"\"\n",
    "#     embed_dim: output dimension for each position\n",
    "#     pos: a list of positions to be encoded: size (M,)\n",
    "#     out: (M, D)\n",
    "#     \"\"\"\n",
    "#     assert embed_dim % 2 == 0\n",
    "#     omega = np.arange(embed_dim // 2, dtype=float)\n",
    "#     omega /= embed_dim / 2.\n",
    "#     omega = 1. / 10000**omega  # (D/2,)\n",
    "\n",
    "#     pos = pos.reshape(-1)  # (M,)\n",
    "#     out = np.einsum('m,d->md', pos, omega)  # (M, D/2), outer product\n",
    "\n",
    "#     emb_sin = np.sin(out) # (M, D/2)\n",
    "#     emb_cos = np.cos(out) # (M, D/2)\n",
    "\n",
    "#     emb = np.concatenate([emb_sin, emb_cos], axis=1)  # (M, D)\n",
    "#     return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23c7e807",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_pos_embed(model, checkpoint_model):\n",
    "    if 'pos_embed' in checkpoint_model:\n",
    "        pos_embed_checkpoint = checkpoint_model['pos_embed']\n",
    "        embedding_size = pos_embed_checkpoint.shape[-1]\n",
    "        num_patches = model.patch_embed.num_patches\n",
    "        num_extra_tokens = model.pos_embed.shape[-2] - num_patches\n",
    "        # height (== width) for the checkpoint position embedding\n",
    "        orig_size = int((pos_embed_checkpoint.shape[-2] - num_extra_tokens) ** 0.5)\n",
    "        # height (== width) for the new position embedding\n",
    "        new_size = int(num_patches ** 0.5)\n",
    "        # class_token and dist_token are kept unchanged\n",
    "        if orig_size != new_size:\n",
    "            print(\"Position interpolate from %dx%d to %dx%d\" % (orig_size, orig_size, new_size, new_size))\n",
    "            extra_tokens = pos_embed_checkpoint[:, :num_extra_tokens]\n",
    "            # only the position tokens are interpolated\n",
    "            pos_tokens = pos_embed_checkpoint[:, num_extra_tokens:]\n",
    "            pos_tokens = pos_tokens.reshape(-1, orig_size, orig_size, embedding_size).permute(0, 3, 1, 2)\n",
    "            pos_tokens = torch.nn.functional.interpolate(\n",
    "                pos_tokens, size=(new_size, new_size), mode='bicubic', align_corners=False)\n",
    "            pos_tokens = pos_tokens.permute(0, 2, 3, 1).flatten(1, 2)\n",
    "            new_pos_embed = torch.cat((extra_tokens, pos_tokens), dim=1)\n",
    "            checkpoint_model['pos_embed'] = new_pos_embed\n",
    "def timestep_embedding(timesteps, dim, max_period=10000):\n",
    "    \"\"\"\n",
    "    Create sinusoidal timestep embeddings.\n",
    "\n",
    "    :param timesteps: a 1-D Tensor of N indices, one per batch element.\n",
    "                      These may be fractional.\n",
    "    :param dim: the dimension of the output.\n",
    "    :param max_period: controls the minimum frequency of the embeddings.\n",
    "    :return: an [N x dim] Tensor of positional embeddings.\n",
    "    \"\"\"\n",
    "    half = dim // 2\n",
    "    freqs = torch.exp(\n",
    "        -math.log(max_period) * torch.arange(start=0, end=half, dtype=torch.float32) / half\n",
    "    ).to(device=timesteps.device)\n",
    "    args = timesteps[:, None].float() * freqs[None]\n",
    "    embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n",
    "    if dim % 2:\n",
    "        embedding = torch.cat([embedding, torch.zeros_like(embedding[:, :1])], dim=-1)\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e948c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MaskedAutoencoderViT(nn.Module):\n",
    "#     \"\"\" Masked Autoencoder with VisionTransformer backbone\n",
    "#     \"\"\"\n",
    "#     def __init__(self, img_size=224, patch_size=16, in_chans=3,\n",
    "#                  embed_dim=1024, depth=24, num_heads=16,\n",
    "#                  qkv_bias=True, qk_scale=None,\n",
    "#                  decoder_embed_dim=512, decoder_depth=8, decoder_num_heads=16,\n",
    "#                  mlp_ratio=4., mlp_time_embed=False, norm_layer=nn.LayerNorm, \n",
    "#                  num_classes=None, norm_pix_loss=False,\n",
    "#                  use_checkpoint=False, conv=True, skip=False):\n",
    "#         super().__init__()\n",
    "\n",
    "#         # --------------------------------------------------------------------------\n",
    "#         # MAE encoder specifics\n",
    "#         self.embed_dim = embed_dim\n",
    "#         self.num_classes = num_classes\n",
    "#         self.patch_embed = PatchEmbed(img_size, patch_size, in_chans, embed_dim)\n",
    "#         num_patches = self.patch_embed.num_patches\n",
    "#         self.time_embed = nn.Sequential(\n",
    "#             nn.Linear(embed_dim, 4 * embed_dim),\n",
    "#             nn.SiLU(),\n",
    "#             nn.Linear(4 * embed_dim, embed_dim),\n",
    "#         ) if mlp_time_embed else nn.Identity()\n",
    "#         self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "#         if self.num_classes is not None:\n",
    "#             self.label_emb = nn.Embedding(self.num_classes, embed_dim)\n",
    "#             self.extras = 2\n",
    "#         else:\n",
    "#             self.extras = 1\n",
    "        \n",
    "#         self.pos_embed = nn.Parameter(torch.zeros(1, self.extras + num_patches, embed_dim), requires_grad=False)  # fixed sin-cos embedding\n",
    "\n",
    "#         self.in_blocks = nn.ModuleList([\n",
    "#             Block(\n",
    "#                 dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "#                 norm_layer=norm_layer, use_checkpoint=use_checkpoint)\n",
    "#             for _ in range(depth // 2)])\n",
    "\n",
    "#         self.norm = norm_layer(embed_dim)\n",
    "#         self.mid_block = nn.Sequential(\n",
    "#             Block(\n",
    "#                 dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "#                 norm_layer=norm_layer, use_checkpoint=use_checkpoint)\n",
    "#         )\n",
    "#         # --------------------------------------------------------------------------\n",
    "\n",
    "#         # --------------------------------------------------------------------------\n",
    "#         # MAE decoder specifics\n",
    "#         self.decoder_embed = nn.Linear(embed_dim, decoder_embed_dim, bias=True)\n",
    "\n",
    "#         self.mask_token = nn.Parameter(torch.zeros(1, 1, decoder_embed_dim))\n",
    "\n",
    "#         self.decoder_pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, decoder_embed_dim), requires_grad=False)  # fixed sin-cos embedding\n",
    "\n",
    "#         self.out_blocks = nn.ModuleList([\n",
    "#             Block(\n",
    "#                 dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "#                 norm_layer=norm_layer, skip=skip, use_checkpoint=use_checkpoint)\n",
    "#             for _ in range(depth // 2)])\n",
    "\n",
    "#         self.decoder_norm = norm_layer(decoder_embed_dim)\n",
    "#         self.decoder_pred = nn.Linear(decoder_embed_dim, patch_size**2 * in_chans, bias=True) # decoder to patch\n",
    "#         # --------------------------------------------------------------------------\n",
    "\n",
    "#         self.norm_pix_loss = norm_pix_loss\n",
    "\n",
    "#         self.initialize_weights()\n",
    "\n",
    "#     def initialize_weights(self):\n",
    "#         # initialization\n",
    "#         # initialize (and freeze) pos_embed by sin-cos embedding\n",
    "#         pos_embed = get_2d_sincos_pos_embed(self.pos_embed.shape[-1], int(self.patch_embed.num_patches**.5), cls_token=True)\n",
    "#         self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))\n",
    "\n",
    "#         decoder_pos_embed = get_2d_sincos_pos_embed(self.decoder_pos_embed.shape[-1], int(self.patch_embed.num_patches**.5), cls_token=True)\n",
    "#         self.decoder_pos_embed.data.copy_(torch.from_numpy(decoder_pos_embed).float().unsqueeze(0))\n",
    "\n",
    "#         # initialize patch_embed like nn.Linear (instead of nn.Conv2d)\n",
    "#         w = self.patch_embed.proj.weight.data\n",
    "#         torch.nn.init.xavier_uniform_(w.view([w.shape[0], -1]))\n",
    "\n",
    "#         # timm's trunc_normal_(std=.02) is effectively normal_(std=0.02) as cutoff is too big (2.)\n",
    "#         torch.nn.init.normal_(self.cls_token, std=.02)\n",
    "#         torch.nn.init.normal_(self.mask_token, std=.02)\n",
    "\n",
    "#         # initialize nn.Linear and nn.LayerNorm\n",
    "#         self.apply(self._init_weights)\n",
    "\n",
    "#     def _init_weights(self, m):\n",
    "#         if isinstance(m, nn.Linear):\n",
    "#             # we use xavier_uniform following official JAX ViT:\n",
    "#             torch.nn.init.xavier_uniform_(m.weight)\n",
    "#             if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "#                 nn.init.constant_(m.bias, 0)\n",
    "#         elif isinstance(m, nn.LayerNorm):\n",
    "#             nn.init.constant_(m.bias, 0)\n",
    "#             nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "#     def patchify(self, imgs):\n",
    "#         \"\"\"\n",
    "#         imgs: (N, 3, H, W)\n",
    "#         x: (N, L, patch_size**2 *3)\n",
    "#         \"\"\"\n",
    "#         p = self.patch_embed.patch_size[0]\n",
    "#         assert imgs.shape[2] == imgs.shape[3] and imgs.shape[2] % p == 0\n",
    "\n",
    "#         h = w = imgs.shape[2] // p\n",
    "#         x = imgs.reshape(shape=(imgs.shape[0], 3, h, p, w, p))\n",
    "#         x = torch.einsum('nchpwq->nhwpqc', x)\n",
    "#         x = x.reshape(shape=(imgs.shape[0], h * w, p**2 * 3))\n",
    "#         return x\n",
    "\n",
    "#     def unpatchify(self, x):\n",
    "#         \"\"\"\n",
    "#         x: (N, L, patch_size**2 *3)\n",
    "#         imgs: (N, 3, H, W)\n",
    "#         \"\"\"\n",
    "#         p = self.patch_embed.patch_size[0]\n",
    "#         h = w = int(x.shape[1]**.5)\n",
    "#         assert h * w == x.shape[1]\n",
    "        \n",
    "#         x = x.reshape(shape=(x.shape[0], h, w, p, p, 3))\n",
    "#         x = torch.einsum('nhwpqc->nchpwq', x)\n",
    "#         imgs = x.reshape(shape=(x.shape[0], 3, h * p, h * p))\n",
    "#         return imgs\n",
    "\n",
    "#     def random_masking(self, x, mask_ratio):\n",
    "#         \"\"\"\n",
    "#         Perform per-sample random masking by per-sample shuffling.\n",
    "#         Per-sample shuffling is done by argsort random noise.\n",
    "#         x: [N, L, D], sequence\n",
    "#         \"\"\"\n",
    "#         N, L, D = x.shape  # batch, length, dim\n",
    "#         len_keep = int(L * (1 - mask_ratio))\n",
    "        \n",
    "#         noise = torch.rand(N, L, device=x.device)  # noise in [0, 1]\n",
    "        \n",
    "#         # sort noise for each sample\n",
    "#         ids_shuffle = torch.argsort(noise, dim=1)  # ascend: small is keep, large is remove\n",
    "#         ids_restore = torch.argsort(ids_shuffle, dim=1)\n",
    "\n",
    "#         # keep the first subset\n",
    "#         ids_keep = ids_shuffle[:, :len_keep]\n",
    "#         x_masked = torch.gather(x, dim=1, index=ids_keep.unsqueeze(-1).repeat(1, 1, D))\n",
    "\n",
    "#         # generate the binary mask: 0 is keep, 1 is remove\n",
    "#         mask = torch.ones([N, L], device=x.device)\n",
    "#         mask[:, :len_keep] = 0\n",
    "#         # unshuffle to get the binary mask\n",
    "#         mask = torch.gather(mask, dim=1, index=ids_restore)\n",
    "\n",
    "#         return x_masked, mask, ids_restore\n",
    "\n",
    "#     def forward_encoder(self, x, timesteps, mask_ratio, y=None):\n",
    "#         # embed patches\n",
    "#         print(x.shape)\n",
    "#         x = self.patch_embed(x)\n",
    "#         print(x.shape)\n",
    "#         # add pos embed w/o cls token\n",
    "#         B, L, D = x.shape\n",
    "# #         print(x)\n",
    "#         time_token = self.time_embed(timestep_embedding(timesteps, self.embed_dim))\n",
    "#         time_token = time_token.unsqueeze(dim=1)\n",
    "#         x = torch.cat((time_token, x), dim=1)\n",
    "#         if y is not None:\n",
    "#             label_emb = self.label_emb(y)\n",
    "#             label_emb = label_emb.unsqueeze(dim=1)\n",
    "#             x = torch.cat((label_emb, x), dim=1)\n",
    "# #             print(\"Yess\")\n",
    "#         x = x + self.pos_embed\n",
    "#         print(x.shape)\n",
    "#         # masking: length -> length * mask_ratio\n",
    "#         x, mask, ids_restore = self.random_masking(x, mask_ratio)\n",
    "#         print(x.shape)\n",
    "#         print(\"mask\", mask.shape)\n",
    "#         skips = []\n",
    "#         for blk in self.in_blocks:\n",
    "#             x = blk(x)\n",
    "#             skips.append(x)\n",
    "# #         bf = x\n",
    "#         x = self.mid_block(x)\n",
    "#         x = self.norm(x)\n",
    "#         print(x.shape)\n",
    "#         return x, mask, ids_restore\n",
    "\n",
    "#     def forward_decoder(self, x, ids_restore):\n",
    "#         # embed tokens\n",
    "#         print(x.shape)\n",
    "#         x = self.decoder_embed(x)\n",
    "#         print(x.shape)\n",
    "#         # append mask tokens to sequence\n",
    "#         mask_tokens = self.mask_token.repeat(x.shape[0], ids_restore.shape[1] + 1 - x.shape[1], 1)\n",
    "#         print(mask_tokens.shape)\n",
    "#         x_ = torch.cat([x[:, 1:, :], mask_tokens], dim=1)  # no cls token\n",
    "#         print(x_.shape)\n",
    "#         x_ = torch.gather(x_, dim=1, index=ids_restore.unsqueeze(-1).repeat(1, 1, x.shape[2]))  # unshuffle\n",
    "#         print(x_.shape)\n",
    "#         x = torch.cat([x[:, :1, :], x_], dim=1)  # append cls token\n",
    "#         print(x.shape)\n",
    "\n",
    "#         # add pos embed\n",
    "#         x = x + self.decoder_pos_embed\n",
    "#         print(x.shape)\n",
    "#         # apply Transformer blocks\n",
    "#         for blk in self.out_blocks:\n",
    "#             x = blk(x)\n",
    "#         x = self.decoder_norm(x)\n",
    "#         print(x.shape)\n",
    "#         # predictor projection\n",
    "#         x = self.decoder_pred(x)\n",
    "#         print(x.shape)\n",
    "#         # remove cls token\n",
    "#         x = x[:, 1:, :]\n",
    "#         print(x.shape)\n",
    "#         return x\n",
    "\n",
    "#     def forward_loss(self, imgs, pred, mask):\n",
    "#         \"\"\"\n",
    "#         imgs: [N, 3, H, W]\n",
    "#         pred: [N, L, p*p*3]\n",
    "#         mask: [N, L], 0 is keep, 1 is remove, \n",
    "#         \"\"\"\n",
    "#         target = self.patchify(imgs)\n",
    "#         if self.norm_pix_loss:\n",
    "#             mean = target.mean(dim=-1, keepdim=True)\n",
    "#             var = target.var(dim=-1, keepdim=True)\n",
    "#             target = (target - mean) / (var + 1.e-6)**.5\n",
    "\n",
    "#         loss = (pred - target) ** 2\n",
    "#         loss = loss.mean(dim=-1)  # [N, L], mean loss per patch\n",
    "\n",
    "#         loss = (loss * mask).sum() / mask.sum()  # mean loss on removed patches\n",
    "#         return loss\n",
    "\n",
    "#     def forward(self, imgs, timesteps, mask_ratio=0.75, y=None):\n",
    "#         latent, mask, ids_restore = self.forward_encoder(imgs, timesteps, mask_ratio, y=None)\n",
    "#         pred = self.forward_decoder(latent, ids_restore)  # [N, L, p*p*3]\n",
    "#         loss = self.forward_loss(imgs, pred, mask)\n",
    "#         return loss, pred, mask\n",
    "\n",
    "\n",
    "# def mae_vit_base_patch16_dec512d8b(**kwargs):\n",
    "#     model = MaskedAutoencoderViT(\n",
    "#         patch_size=16, embed_dim=768, depth=12, num_heads=12,\n",
    "#         decoder_embed_dim=512, decoder_depth=8, decoder_num_heads=16,\n",
    "#         mlp_ratio=4, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "#     return model\n",
    "\n",
    "\n",
    "# def mae_vit_large_patch16_dec512d8b(**kwargs):\n",
    "#     model = MaskedAutoencoderViT(\n",
    "#         patch_size=16, embed_dim=1024, depth=24, num_heads=16,\n",
    "#         decoder_embed_dim=512, decoder_depth=8, decoder_num_heads=16,\n",
    "#         mlp_ratio=4, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "#     return model\n",
    "\n",
    "\n",
    "# def mae_vit_huge_patch14_dec512d8b(**kwargs):\n",
    "#     model = MaskedAutoencoderViT(\n",
    "#         patch_size=14, embed_dim=1280, depth=32, num_heads=16,\n",
    "#         decoder_embed_dim=512, decoder_depth=8, decoder_num_heads=16,\n",
    "#         mlp_ratio=4, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "#     return model\n",
    "\n",
    "\n",
    "# # set recommended archs\n",
    "# mae_vit_base_patch16 = mae_vit_base_patch16_dec512d8b  # decoder: 512 dim, 8 blocks\n",
    "# mae_vit_large_patch16 = mae_vit_large_patch16_dec512d8b  # decoder: 512 dim, 8 blocks\n",
    "# mae_vit_huge_patch14 = mae_vit_huge_patch14_dec512d8b  # decoder: 512 dim, 8 blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38af83fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = mae_vit_base_patch16(img_size = 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35b9a39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = torch.rand(1, 3, 32, 32)\n",
    "t_batch = torch.tensor([1], device=x_test.device).repeat(x_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b2661a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_batch = torch.tensor([1], device=x_test.device).repeat(x_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b7193840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_test = model(x_test, t_batch, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1624d6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84370eef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ed289531",
   "metadata": {},
   "outputs": [],
   "source": [
    "import maskdit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "658a305f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = maskdit.DiT_B_2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f6e29f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = model(x_test, t_batch, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "54dd2840",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 32, 32])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test['x'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09042e1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
