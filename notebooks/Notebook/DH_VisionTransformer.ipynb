{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2be95e3c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'complement_idx' from 'helpers' (C:\\Users\\Admin\\Documents\\Anomaly Detection\\AnoDDPM\\helpers.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 36\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtimm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregistry\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m register_model\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtimm\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhelpers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m complement_idx\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01meinops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m rearrange, reduce, repeat\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01meinops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Rearrange, Reduce\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'complement_idx' from 'helpers' (C:\\Users\\Admin\\Documents\\Anomaly Detection\\AnoDDPM\\helpers.py)"
     ]
    }
   ],
   "source": [
    "\"\"\" Vision Transformer (ViT) in PyTorch\n",
    "A PyTorch implement of Vision Transformers as described in:\n",
    "'An Image Is Worth 16 x 16 Words: Transformers for Image Recognition at Scale'\n",
    "    - https://arxiv.org/abs/2010.11929\n",
    "`How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers`\n",
    "    - https://arxiv.org/abs/2106.10270\n",
    "The official jax code is released and available at https://github.com/google-research/vision_transformer\n",
    "DeiT model defs and weights from https://github.com/facebookresearch/deit,\n",
    "paper `DeiT: Data-efficient Image Transformers` - https://arxiv.org/abs/2012.12877\n",
    "Acknowledgments:\n",
    "* The paper authors for releasing code and weights, thanks!\n",
    "* I fixed my class token impl based on Phil Wang's https://github.com/lucidrains/vit-pytorch ... check it out\n",
    "for some einops/einsum fun\n",
    "* Simple transformer style inspired by Andrej Karpathy's https://github.com/karpathy/minGPT\n",
    "* Bert reference code checks against Huggingface Transformers and Tensorflow Bert\n",
    "Hacked together by / Copyright 2021 Ross Wightman\n",
    "# ------------------------------------------\n",
    "# Modification:\n",
    "# Added code for DHVT -- Copyright 2022 Zhiying Lu\n",
    "\"\"\"\n",
    "import math\n",
    "import logging\n",
    "from functools import partial\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD, IMAGENET_INCEPTION_MEAN, IMAGENET_INCEPTION_STD\n",
    "from timm.models.helpers import build_model_with_cfg, named_apply, adapt_input_conv\n",
    "from timm.models.layers import trunc_normal_, lecun_normal_, to_2tuple\n",
    "from timm.models.registry import register_model\n",
    "import timm\n",
    "\n",
    "from helpers import complement_idx\n",
    "from einops import rearrange, reduce, repeat\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "\n",
    "_logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def _cfg(url='', **kwargs):\n",
    "    return {\n",
    "        'url': url,\n",
    "        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': None,\n",
    "        'crop_pct': .9, 'interpolation': 'bicubic', 'fixed_input_size': True,\n",
    "        'mean': IMAGENET_INCEPTION_MEAN, 'std': IMAGENET_INCEPTION_STD,\n",
    "        'first_conv': 'patch_embed.proj', 'classifier': 'head',\n",
    "        **kwargs\n",
    "    }\n",
    "\n",
    "\n",
    "default_cfgs = {\n",
    "    # deit models (FB weights)\n",
    "    'deit_tiny_patch16_224': _cfg(\n",
    "        url='https://dl.fbaipublicfiles.com/deit/deit_tiny_patch16_224-a1311bcf.pth',\n",
    "        mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD),\n",
    "    'deit_small_patch16_224': _cfg(\n",
    "        url='https://dl.fbaipublicfiles.com/deit/deit_small_patch16_224-cd65a155.pth',\n",
    "        mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD),\n",
    "}\n",
    "\n",
    "\n",
    "def drop_path(x, drop_prob: float = 0., training: bool = False):\n",
    "    \"\"\"Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\n",
    "    This is the same as the DropConnect impl I created for EfficientNet, etc networks, however,\n",
    "    the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...\n",
    "    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ... I've opted for\n",
    "    changing the layer and argument names to 'drop path' rather than mix DropConnect as a layer name and use\n",
    "    'survival rate' as the argument.\n",
    "    \"\"\"\n",
    "    if drop_prob == 0. or not training:\n",
    "        return x\n",
    "    keep_prob = 1 - drop_prob\n",
    "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
    "    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
    "    random_tensor.floor_()  # binarize\n",
    "    output = x.div(keep_prob) * random_tensor\n",
    "    return output\n",
    "\n",
    "\n",
    "class DropPath(nn.Module):\n",
    "    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n",
    "    \"\"\"\n",
    "    def __init__(self, drop_prob=None):\n",
    "        super(DropPath, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        return drop_path(x, self.drop_prob, self.training)\n",
    "\n",
    "\n",
    "# vanilla MLP\n",
    "class Mlp(nn.Module):\n",
    "    \"\"\" MLP as used in Vision Transformer, MLP-Mixer and related networks\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "# The proposed DAFF\n",
    "class DAFF(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.,\n",
    "                 kernel_size=3, with_bn=True):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        # pointwise\n",
    "        self.conv1 = nn.Conv2d(in_features, hidden_features, kernel_size=1, stride=1, padding=0)\n",
    "        # depthwise\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            hidden_features, hidden_features, kernel_size=kernel_size, stride=1,\n",
    "            padding=(kernel_size - 1) // 2, groups=hidden_features)\n",
    "        \n",
    "        # pointwise\n",
    "        self.conv3 = nn.Conv2d(hidden_features, out_features, kernel_size=1, stride=1, padding=0)\n",
    "        self.act = act_layer()\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm2d(hidden_features)\n",
    "        self.bn2 = nn.BatchNorm2d(hidden_features)\n",
    "        self.bn3 = nn.BatchNorm2d(out_features)\n",
    "        \n",
    "        # The reduction ratio is always set to 4\n",
    "        self.squeeze = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.compress = nn.Linear(in_features, in_features//4)\n",
    "        self.excitation = nn.Linear(in_features//4, in_features)\n",
    "                \n",
    "    def forward(self, x):\n",
    "        B, N, C = x.size()\n",
    "        cls_token, tokens = torch.split(x, [1, N - 1], dim=1)\n",
    "        x = tokens.reshape(B, int(math.sqrt(N - 1)), int(math.sqrt(N - 1)), C).permute(0, 3, 1, 2)\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.act(x)\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.act(x)\n",
    "        x = shortcut + x\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "\n",
    "        weight = self.squeeze(x).flatten(1).reshape(B, 1, C)\n",
    "        weight = self.excitation(self.act(self.compress(weight)))\n",
    "        cls_token = cls_token * weight\n",
    "        \n",
    "        tokens = x.flatten(2).permute(0, 2, 1)\n",
    "        out = torch.cat((cls_token, tokens), dim=1)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    \n",
    "# vanilla non-overlapping patch embedding\n",
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\" 2D Image to Patch Embedding\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768, norm_layer=None, flatten=True):\n",
    "        super().__init__()\n",
    "        img_size = to_2tuple(img_size)\n",
    "        patch_size = to_2tuple(patch_size)\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.grid_size = (img_size[0] // patch_size[0], img_size[1] // patch_size[1])\n",
    "        self.num_patches = self.grid_size[0] * self.grid_size[1]\n",
    "        self.flatten = flatten\n",
    "\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity()\n",
    "        self.norm_layer = norm_layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)\n",
    "        if self.flatten:\n",
    "            x = x.flatten(2).transpose(1, 2)  # BCHW -> BNC\n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "# The proposed HI-MHSA\n",
    "class HI_Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, attn_drop=0., proj_drop=0.):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "        \n",
    "        self.act = nn.GELU()\n",
    "        self.ht_proj = nn.Linear(dim//self.num_heads, dim,bias=True)\n",
    "        self.ht_norm = nn.LayerNorm(dim//self.num_heads)\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, self.num_heads, dim))\n",
    "        trunc_normal_(self.pos_embed, std=.02)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        H = W =int(math.sqrt(N-1))\n",
    "\n",
    "        # head token\n",
    "        head_pos = self.pos_embed.expand(x.shape[0], -1, -1)\n",
    "        x_ = x.reshape(B, -1, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3) \n",
    "        x_ = x_.mean(dim=2)  # now the shape is [B, h, 1, d//h]\n",
    "        x_ = self.ht_proj(x_).reshape(B, -1, self.num_heads, C // self.num_heads)\n",
    "        x_ = self.act(self.ht_norm(x_)).flatten(2)\n",
    "        x_ = x_ + head_pos\n",
    "        x = torch.cat([x, x_], dim=1)\n",
    "        \n",
    "        # normal mhsa\n",
    "        qkv = self.qkv(x).reshape(B, N+self.num_heads, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]   # make torchscript happy (cannot use tensor as tuple)\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "        \n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N+self.num_heads, C)\n",
    "        x = self.proj(x)\n",
    "        \n",
    "        # merge head tokens into cls token\n",
    "        cls, patch, ht = torch.split(x, [1, N-1, self.num_heads], dim=1)\n",
    "        cls = cls + torch.mean(ht, dim=1, keepdim=True)\n",
    "        x = torch.cat([cls, patch], dim=1)\n",
    "\n",
    "        x = self.proj_drop(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    \n",
    "# vanilla MHSA\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, attn_drop=0., proj_drop=0.):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = head_dim ** -0.5\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "                \n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        H = W =int(math.sqrt(N-1))\n",
    "\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]   # make torchscript happy (cannot use tensor as tuple)\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    \n",
    "# vanilla ViT Encoder Block\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, drop=0., attn_drop=0., qk_scale=None, \n",
    "                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = Attention(dim, num_heads=num_heads, qkv_bias=qkv_bias,attn_drop=attn_drop, proj_drop=drop)\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        \n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "        self.mlp_hidden_dim = mlp_hidden_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        x = x + self.drop_path(self.attn(self.norm1(x)))\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        return x\n",
    "    \n",
    "    \n",
    "# DHVT Encoder block with HI-MHSA and SOPE\n",
    "class DHVT_Block(nn.Module):\n",
    "\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, drop=0., attn_drop=0., qk_scale=None, \n",
    "                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = HI_Attention(dim, num_heads=num_heads, qkv_bias=qkv_bias,attn_drop=attn_drop, proj_drop=drop)\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        \n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = DAFF(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop, kernel_size=3)\n",
    "        self.mlp_hidden_dim = mlp_hidden_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        x = x + self.drop_path(self.attn(self.norm1(x)))\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        return x\n",
    "\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return torch.nn.Sequential(\n",
    "        nn.Conv2d(\n",
    "            in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False\n",
    "        ),\n",
    "        nn.SyncBatchNorm(out_planes)\n",
    "        #nn.BatchNorm2d(out_planes)\n",
    "    )\n",
    "\n",
    "\n",
    "# Affine on Conv-style features with shape of [B, C, H, W]\n",
    "class Affine(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.alpha = nn.Parameter(torch.ones([1, dim, 1, 1]),requires_grad=True)\n",
    "        self.beta = nn.Parameter(torch.zeros([1, dim, 1, 1]),requires_grad=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x * self.alpha + self.beta\n",
    "        return x    \n",
    "\n",
    "\n",
    "# The proposed SOPE\n",
    "class ConvPatchEmbed(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768, init_values=1e-2):\n",
    "        super().__init__()\n",
    "        ori_img_size = img_size\n",
    "        img_size = to_2tuple(img_size)\n",
    "        patch_size = to_2tuple(patch_size)\n",
    "        num_patches = (img_size[1] // patch_size[1]) * (img_size[0] // patch_size[0])\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "        \n",
    "        if patch_size[0] == 16:\n",
    "            self.proj = torch.nn.Sequential(\n",
    "                conv3x3(3, embed_dim // 8, 2),\n",
    "                nn.GELU(),\n",
    "                conv3x3(embed_dim // 8, embed_dim // 4, 2),\n",
    "                nn.GELU(),\n",
    "                conv3x3(embed_dim // 4, embed_dim // 2, 2),\n",
    "                nn.GELU(),\n",
    "                conv3x3(embed_dim // 2, embed_dim, 2),\n",
    "            )\n",
    "        elif patch_size[0] == 4:  \n",
    "            self.proj = torch.nn.Sequential(\n",
    "                conv3x3(3, embed_dim // 2, 2),\n",
    "                nn.GELU(),\n",
    "                conv3x3(embed_dim // 2, embed_dim, 2),\n",
    "            )\n",
    "        elif patch_size[0] == 2:  \n",
    "            self.proj = torch.nn.Sequential(\n",
    "                conv3x3(3, embed_dim, 2),\n",
    "                nn.GELU(),\n",
    "            )\n",
    "        else:\n",
    "            raise(\"For convolutional projection, patch size has to be in [2, 4, 16]\")\n",
    "        self.pre_affine = Affine(3)\n",
    "        self.post_affine = Affine(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape \n",
    "        \n",
    "        x = self.pre_affine(x)\n",
    "        x = self.proj(x)\n",
    "        x = self.post_affine(x)\n",
    "\n",
    "        Hp, Wp = x.shape[2], x.shape[3]\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "\n",
    "        return x\n",
    "    \n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    \"\"\" Vision Transformer\n",
    "    A PyTorch impl of : `An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale`\n",
    "        - https://arxiv.org/abs/2010.11929\n",
    "    Includes distillation token & head support for `DeiT: Data-efficient Image Transformers`\n",
    "        - https://arxiv.org/abs/2012.12877\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, img_size=32, patch_size=16, in_chans=3, num_classes=100, embed_dim=768, depth=12,\n",
    "                 num_heads=12, mlp_ratio=4., qkv_bias=True, representation_size=None, distilled=False,\n",
    "                 drop_rate=0., attn_drop_rate=0., drop_path_rate=0., embed_layer=PatchEmbed, norm_layer=None,\n",
    "                 act_layer=None, weight_init='',apply_dhvt=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img_size (int, tuple): input image size\n",
    "            patch_size (int, tuple): patch size\n",
    "            in_chans (int): number of input channels\n",
    "            num_classes (int): number of classes for classification head\n",
    "            embed_dim (int): embedding dimension\n",
    "            depth (int): depth of transformer\n",
    "            num_heads (int): number of attention heads\n",
    "            mlp_ratio (int): ratio of mlp hidden dim to embedding dim\n",
    "            qkv_bias (bool): enable bias for qkv if True\n",
    "            representation_size (Optional[int]): enable and set representation layer (pre-logits) to this value if set\n",
    "            distilled (bool): model includes a distillation token and head as in DeiT models\n",
    "            drop_rate (float): dropout rate\n",
    "            attn_drop_rate (float): attention dropout rate\n",
    "            drop_path_rate (float): stochastic depth rate\n",
    "            embed_layer (nn.Module): patch embedding layer\n",
    "            norm_layer: (nn.Module): normalization layer\n",
    "            weight_init: (str): weight init scheme\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.depth = depth\n",
    "        self.num_classes = num_classes\n",
    "        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n",
    "        self.num_tokens = 2 if distilled else 1\n",
    "        norm_layer = norm_layer or partial(nn.LayerNorm, eps=1e-6)\n",
    "        act_layer = act_layer or nn.GELU\n",
    "        self.apply_dhvt = apply_dhvt\n",
    "        \n",
    "        # Patch Embedding\n",
    "        if self.apply_dhvt:\n",
    "            self.patch_embed = ConvPatchEmbed(img_size=img_size, embed_dim=embed_dim, patch_size=patch_size)\n",
    "        else:\n",
    "            self.patch_embed = embed_layer(img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n",
    "            self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + self.num_tokens, embed_dim))\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.dist_token = nn.Parameter(torch.zeros(1, 1, embed_dims[-1])) if distilled else None\n",
    "\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
    " \n",
    "        if self.apply_dhvt:\n",
    "            self.blocks = nn.ModuleList([\n",
    "                DHVT_Block(\n",
    "                    dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, drop=drop_rate,\n",
    "                    attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer, act_layer=act_layer)\n",
    "                for i in range(depth)])\n",
    "        else:\n",
    "            self.blocks = nn.ModuleList([\n",
    "                Block(\n",
    "                    dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, drop=drop_rate,\n",
    "                    attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer, act_layer=act_layer)\n",
    "                for i in range(depth)])\n",
    "        \n",
    "        self.norm = norm_layer(embed_dim)\n",
    "\n",
    "        # Representation layer\n",
    "        if representation_size and not distilled:\n",
    "            self.num_features = representation_size\n",
    "            self.pre_logits = nn.Sequential(OrderedDict([\n",
    "                ('fc', nn.Linear(embed_dim, representation_size)),\n",
    "                ('act', nn.Tanh())\n",
    "            ]))\n",
    "        else:\n",
    "            self.pre_logits = nn.Identity()\n",
    "\n",
    "        # Classifier head(s)\n",
    "        self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()\n",
    "        self.head_dist = None\n",
    "        if distilled:\n",
    "            self.head_dist = nn.Linear(self.embed_dim, self.num_classes) if num_classes > 0 else nn.Identity()\n",
    "\n",
    "        self.init_weights(weight_init)\n",
    "\n",
    "    def init_weights(self, mode=''):\n",
    "        assert mode in ('jax', 'jax_nlhb', 'nlhb', '')\n",
    "        head_bias = -math.log(self.num_classes) if 'nlhb' in mode else 0.\n",
    "        if not self.apply_dhvt:\n",
    "            trunc_normal_(self.pos_embed, std=.02)\n",
    "        if self.dist_token is not None:\n",
    "            trunc_normal_(self.dist_token, std=.02)\n",
    "        if mode.startswith('jax'):\n",
    "            # leave cls token as zeros to match jax impl\n",
    "            named_apply(partial(_init_vit_weights, head_bias=head_bias, jax_impl=True), self)\n",
    "        else:\n",
    "            trunc_normal_(self.cls_token, std=.02)\n",
    "            self.apply(_init_vit_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        # this fn left here for compat with downstream users\n",
    "        _init_vit_weights(m)\n",
    "\n",
    "    @torch.jit.ignore()\n",
    "    def load_pretrained(self, checkpoint_path, prefix=''):\n",
    "        _load_weights(self, checkpoint_path, prefix)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {'pos_embed', 'cls_token', 'dist_token'}\n",
    "\n",
    "    def get_classifier(self):\n",
    "        if self.dist_token is None:\n",
    "            return self.head\n",
    "        else:\n",
    "            return self.head, self.head_dist\n",
    "\n",
    "    def reset_classifier(self, num_classes, global_pool=''):\n",
    "        self.num_classes = num_classes\n",
    "        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
    "        if self.num_tokens == 2:\n",
    "            self.head_dist = nn.Linear(self.embed_dim, self.num_classes) if num_classes > 0 else nn.Identity()\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        return \"vit_simple_topk\"\n",
    "    \n",
    "    \n",
    "    def forward_features(self, x):\n",
    "        B, _, h, w = x.shape\n",
    "        x = self.patch_embed(x)\n",
    "        cls_token = self.cls_token.expand(x.shape[0], -1, -1)  # stole cls_tokens impl from Phil Wang, thanks\n",
    "        if self.dist_token is None:\n",
    "            x = torch.cat((cls_token, x), dim=1)\n",
    "        else:\n",
    "            x = torch.cat((cls_token, self.dist_token.expand(x.shape[0], -1, -1), x), dim=1)\n",
    "\n",
    "        # for input with another resolution, interpolate the positional embedding.\n",
    "        # used for finetining a ViT on images with larger size.\n",
    "        # only applied in vanilla ViT\n",
    "        if not self.apply_dhvt:\n",
    "            pos_embed = self.pos_embed\n",
    "            if x.shape[1] != pos_embed.shape[1]:\n",
    "                assert h == w  # for simplicity assume h == w\n",
    "                real_pos = pos_embed[:, self.num_tokens:]\n",
    "                hw = int(math.sqrt(real_pos.shape[1]))\n",
    "                true_hw = int(math.sqrt(x.shape[1] - self.num_tokens))\n",
    "                real_pos = real_pos.transpose(1, 2).reshape(1, self.embed_dim, hw, hw)\n",
    "                new_pos = F.interpolate(real_pos, size=true_hw, mode='bicubic', align_corners=False)\n",
    "                new_pos = new_pos.reshape(1, self.embed_dim, -1).transpose(1, 2)\n",
    "                pos_embed = torch.cat([pos_embed[:, :self.num_tokens], new_pos], dim=1)\n",
    "\n",
    "            x = self.pos_drop(x + pos_embed)\n",
    "        \n",
    "        for i, blk in enumerate(self.blocks):\n",
    "            x  = blk(x)\n",
    "            \n",
    "        x = self.norm(x)\n",
    "        \n",
    "        if self.dist_token is None:\n",
    "            return self.pre_logits(x[:, 0])\n",
    "        else:\n",
    "            return x[:, 0], x[:, 1]\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.forward_features(x)\n",
    "        if self.head_dist is not None:\n",
    "            x, x_dist = self.head(x[0]), self.head_dist(x[1])  # x must be a tuple\n",
    "            if self.training and not torch.jit.is_scripting():\n",
    "                # during inference, return the average of both classifier predictions\n",
    "                return x, x_dist\n",
    "            else:\n",
    "                return (x + x_dist) / 2\n",
    "        else:\n",
    "            x = self.head(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def _init_vit_weights(module: nn.Module, name: str = '', head_bias: float = 0., jax_impl: bool = False):\n",
    "    \"\"\" ViT weight initialization\n",
    "    * When called without n, head_bias, jax_impl args it will behave exactly the same\n",
    "      as my original init for compatibility with prev hparam / downstream use cases (ie DeiT).\n",
    "    * When called w/ valid n (module name) and jax_impl=True, will (hopefully) match JAX impl\n",
    "    \"\"\"\n",
    "    if isinstance(module, nn.Linear):\n",
    "        if name.startswith('head'):\n",
    "            nn.init.zeros_(module.weight)\n",
    "            nn.init.constant_(module.bias, head_bias)\n",
    "        elif name.startswith('pre_logits'):\n",
    "            lecun_normal_(module.weight)\n",
    "            nn.init.zeros_(module.bias)\n",
    "        else:\n",
    "            if jax_impl:\n",
    "                nn.init.xavier_uniform_(module.weight)\n",
    "                if module.bias is not None:\n",
    "                    if 'mlp' in name:\n",
    "                        nn.init.normal_(module.bias, std=1e-6)\n",
    "                    else:\n",
    "                        nn.init.zeros_(module.bias)\n",
    "            else:\n",
    "                trunc_normal_(module.weight, std=.02)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.zeros_(module.bias)\n",
    "    elif jax_impl and isinstance(module, nn.Conv2d):\n",
    "        # NOTE conv was left to pytorch default in my original init\n",
    "        lecun_normal_(module.weight)\n",
    "        if module.bias is not None:\n",
    "            nn.init.zeros_(module.bias)\n",
    "    elif isinstance(module, (nn.LayerNorm, nn.GroupNorm, nn.BatchNorm2d)):\n",
    "        nn.init.zeros_(module.bias)\n",
    "        nn.init.ones_(module.weight)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def _load_weights(model: VisionTransformer, checkpoint_path: str, prefix: str = ''):\n",
    "    \"\"\" Load weights from .npz checkpoints for official Google Brain Flax implementation\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "\n",
    "    def _n2p(w, t=True):\n",
    "        if w.ndim == 4 and w.shape[0] == w.shape[1] == w.shape[2] == 1:\n",
    "            w = w.flatten()\n",
    "        if t:\n",
    "            if w.ndim == 4:\n",
    "                w = w.transpose([3, 2, 0, 1])\n",
    "            elif w.ndim == 3:\n",
    "                w = w.transpose([2, 0, 1])\n",
    "            elif w.ndim == 2:\n",
    "                w = w.transpose([1, 0])\n",
    "        return torch.from_numpy(w)\n",
    "\n",
    "    w = np.load(checkpoint_path)\n",
    "    if not prefix and 'opt/target/embedding/kernel' in w:\n",
    "        prefix = 'opt/target/'\n",
    "\n",
    "    if hasattr(model.patch_embed, 'backbone'):\n",
    "        # hybrid\n",
    "        backbone = model.patch_embed.backbone\n",
    "        stem_only = not hasattr(backbone, 'stem')\n",
    "        stem = backbone if stem_only else backbone.stem\n",
    "        stem.conv.weight.copy_(adapt_input_conv(stem.conv.weight.shape[1], _n2p(w[f'{prefix}conv_root/kernel'])))\n",
    "        stem.norm.weight.copy_(_n2p(w[f'{prefix}gn_root/scale']))\n",
    "        stem.norm.bias.copy_(_n2p(w[f'{prefix}gn_root/bias']))\n",
    "        if not stem_only:\n",
    "            for i, stage in enumerate(backbone.stages):\n",
    "                for j, block in enumerate(stage.blocks):\n",
    "                    bp = f'{prefix}block{i + 1}/unit{j + 1}/'\n",
    "                    for r in range(3):\n",
    "                        getattr(block, f'conv{r + 1}').weight.copy_(_n2p(w[f'{bp}conv{r + 1}/kernel']))\n",
    "                        getattr(block, f'norm{r + 1}').weight.copy_(_n2p(w[f'{bp}gn{r + 1}/scale']))\n",
    "                        getattr(block, f'norm{r + 1}').bias.copy_(_n2p(w[f'{bp}gn{r + 1}/bias']))\n",
    "                    if block.downsample is not None:\n",
    "                        block.downsample.conv.weight.copy_(_n2p(w[f'{bp}conv_proj/kernel']))\n",
    "                        block.downsample.norm.weight.copy_(_n2p(w[f'{bp}gn_proj/scale']))\n",
    "                        block.downsample.norm.bias.copy_(_n2p(w[f'{bp}gn_proj/bias']))\n",
    "        embed_conv_w = _n2p(w[f'{prefix}embedding/kernel'])\n",
    "    else:\n",
    "        embed_conv_w = adapt_input_conv(\n",
    "            model.patch_embed.proj.weight.shape[1], _n2p(w[f'{prefix}embedding/kernel']))\n",
    "    model.patch_embed.proj.weight.copy_(embed_conv_w)\n",
    "    model.patch_embed.proj.bias.copy_(_n2p(w[f'{prefix}embedding/bias']))\n",
    "    model.cls_token.copy_(_n2p(w[f'{prefix}cls'], t=False))\n",
    "    pos_embed_w = _n2p(w[f'{prefix}Transformer/posembed_input/pos_embedding'], t=False)\n",
    "    if pos_embed_w.shape != model.pos_embed.shape:\n",
    "        pos_embed_w = resize_pos_embed(  # resize pos embedding when different size from pretrained weights\n",
    "            pos_embed_w, model.pos_embed, getattr(model, 'num_tokens', 1), model.patch_embed.grid_size)\n",
    "    model.pos_embed.copy_(pos_embed_w)\n",
    "    model.norm.weight.copy_(_n2p(w[f'{prefix}Transformer/encoder_norm/scale']))\n",
    "    model.norm.bias.copy_(_n2p(w[f'{prefix}Transformer/encoder_norm/bias']))\n",
    "    if isinstance(model.head, nn.Linear) and model.head.bias.shape[0] == w[f'{prefix}head/bias'].shape[-1]:\n",
    "        model.head.weight.copy_(_n2p(w[f'{prefix}head/kernel']))\n",
    "        model.head.bias.copy_(_n2p(w[f'{prefix}head/bias']))\n",
    "    if isinstance(getattr(model.pre_logits, 'fc', None), nn.Linear) and f'{prefix}pre_logits/bias' in w:\n",
    "        model.pre_logits.fc.weight.copy_(_n2p(w[f'{prefix}pre_logits/kernel']))\n",
    "        model.pre_logits.fc.bias.copy_(_n2p(w[f'{prefix}pre_logits/bias']))\n",
    "    for i, block in enumerate(model.blocks.children()):\n",
    "        block_prefix = f'{prefix}Transformer/encoderblock_{i}/'\n",
    "        mha_prefix = block_prefix + 'MultiHeadDotProductAttention_1/'\n",
    "        block.norm1.weight.copy_(_n2p(w[f'{block_prefix}LayerNorm_0/scale']))\n",
    "        block.norm1.bias.copy_(_n2p(w[f'{block_prefix}LayerNorm_0/bias']))\n",
    "        block.attn.qkv.weight.copy_(torch.cat([\n",
    "            _n2p(w[f'{mha_prefix}{n}/kernel'], t=False).flatten(1).T for n in ('query', 'key', 'value')]))\n",
    "        block.attn.qkv.bias.copy_(torch.cat([\n",
    "            _n2p(w[f'{mha_prefix}{n}/bias'], t=False).reshape(-1) for n in ('query', 'key', 'value')]))\n",
    "        block.attn.proj.weight.copy_(_n2p(w[f'{mha_prefix}out/kernel']).flatten(1))\n",
    "        block.attn.proj.bias.copy_(_n2p(w[f'{mha_prefix}out/bias']))\n",
    "        for r in range(2):\n",
    "            getattr(block.mlp, f'fc{r + 1}').weight.copy_(_n2p(w[f'{block_prefix}MlpBlock_3/Dense_{r}/kernel']))\n",
    "            getattr(block.mlp, f'fc{r + 1}').bias.copy_(_n2p(w[f'{block_prefix}MlpBlock_3/Dense_{r}/bias']))\n",
    "        block.norm2.weight.copy_(_n2p(w[f'{block_prefix}LayerNorm_2/scale']))\n",
    "        block.norm2.bias.copy_(_n2p(w[f'{block_prefix}LayerNorm_2/bias']))\n",
    "\n",
    "\n",
    "def resize_pos_embed(posemb, posemb_new, num_tokens=1, gs_new=()):\n",
    "    # Rescale the grid of position embeddings when loading from state_dict. Adapted from\n",
    "    # https://github.com/google-research/vision_transformer/blob/00883dd691c63a6830751563748663526e811cee/vit_jax/checkpoint.py#L224\n",
    "    _logger.info('Resized position embedding: %s to %s', posemb.shape, posemb_new.shape)\n",
    "    ntok_new = posemb_new.shape[1]\n",
    "    if num_tokens:\n",
    "        posemb_tok, posemb_grid = posemb[:, :num_tokens], posemb[0, num_tokens:]\n",
    "        ntok_new -= num_tokens\n",
    "    else:\n",
    "        posemb_tok, posemb_grid = posemb[:, :0], posemb[0]\n",
    "    gs_old = int(math.sqrt(len(posemb_grid)))\n",
    "    if not len(gs_new):  # backwards compatibility\n",
    "        gs_new = [int(math.sqrt(ntok_new))] * 2\n",
    "    assert len(gs_new) >= 2\n",
    "    _logger.info('Position embedding grid-size from %s to %s', [gs_old, gs_old], gs_new)\n",
    "    posemb_grid = posemb_grid.reshape(1, gs_old, gs_old, -1).permute(0, 3, 1, 2)\n",
    "    posemb_grid = F.interpolate(posemb_grid, size=gs_new, mode='bicubic', align_corners=False)\n",
    "    posemb_grid = posemb_grid.permute(0, 2, 3, 1).reshape(1, gs_new[0] * gs_new[1], -1)\n",
    "    posemb = torch.cat([posemb_tok, posemb_grid], dim=1)\n",
    "    return posemb\n",
    "\n",
    "\n",
    "def checkpoint_filter_fn(state_dict, model):\n",
    "    \"\"\" convert patch embedding weight from manual patchify + linear proj to conv\"\"\"\n",
    "    out_dict = {}\n",
    "    if 'model' in state_dict:\n",
    "        # For deit models\n",
    "        state_dict = state_dict['model']\n",
    "    for k, v in state_dict.items():\n",
    "        if 'patch_embed.proj.weight' in k and len(v.shape) < 4:\n",
    "            # For old models that I trained prior to conv based patchification\n",
    "            O, I, H, W = model.patch_embed.proj.weight.shape\n",
    "            v = v.reshape(O, -1, H, W)\n",
    "        elif k == 'pos_embed' and v.shape != model.pos_embed.shape:\n",
    "            # To resize pos embedding when using model at different size from pretrained weights\n",
    "            v = resize_pos_embed(\n",
    "                v, model.pos_embed, getattr(model, 'num_tokens', 1), model.patch_embed.grid_size)\n",
    "        out_dict[k] = v\n",
    "    return out_dict\n",
    "\n",
    "\n",
    "def _create_vision_transformer(variant, pretrained=False, default_cfg=None, **kwargs):\n",
    "    default_cfg = default_cfg or default_cfgs[variant]\n",
    "    default_cfg.update(kwargs)\n",
    "    if kwargs.get('features_only', None):\n",
    "        raise RuntimeError('features_only not implemented for Vision Transformer models.')\n",
    "\n",
    "    # NOTE this extra code to support handling of repr size for in21k pretrained models\n",
    "    default_num_classes = default_cfg['num_classes']\n",
    "    num_classes = kwargs.get('num_classes', default_num_classes)\n",
    "    repr_size = kwargs.pop('representation_size', None)\n",
    "    if repr_size is not None and num_classes != default_num_classes:\n",
    "        # Remove representation layer if fine-tuning. This may not always be the desired action,\n",
    "        # but I feel better than doing nothing by default for fine-tuning. Perhaps a better interface?\n",
    "        _logger.warning(\"Removing representation layer for fine-tuning.\")\n",
    "        repr_size = None\n",
    "\n",
    "    model = build_model_with_cfg(\n",
    "        VisionTransformer, variant, pretrained,\n",
    "        default_cfg=default_cfg,\n",
    "        representation_size=repr_size,\n",
    "        pretrained_filter_fn=checkpoint_filter_fn,\n",
    "        pretrained_custom_load='npz' in default_cfg['url'],\n",
    "        **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# prototype models\n",
    "\n",
    "\n",
    "@register_model\n",
    "def deit_tiny_patch16_224(pretrained=False, **kwargs):\n",
    "    \"\"\" DeiT-tiny model @ 224x224 from paper (https://arxiv.org/abs/2012.12877).\n",
    "    ImageNet-1k weights from https://github.com/facebookresearch/deit.\n",
    "    \"\"\"\n",
    "    model_kwargs = dict(patch_size=16, embed_dim=192, depth=12, num_heads=3, **kwargs)\n",
    "    model = _create_vision_transformer('deit_tiny_patch16_224', pretrained=pretrained, **model_kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "@register_model\n",
    "def deit_small_patch16_224(pretrained=False, **kwargs):\n",
    "    \"\"\" DeiT-small model @ 224x224 from paper (https://arxiv.org/abs/2012.12877).\n",
    "    ImageNet-1k weights from https://github.com/facebookresearch/deit.\n",
    "    \"\"\"\n",
    "    model_kwargs = dict(patch_size=16, embed_dim=384, depth=12, num_heads=6, **kwargs)\n",
    "    model = _create_vision_transformer('deit_small_patch16_224', pretrained=pretrained, **model_kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# DHVT model variants\n",
    "\n",
    "@register_model\n",
    "def dhvt_tiny_cifar_patch4(pretrained=False, **kwargs):\n",
    "    model_kwargs = dict(img_size=32, patch_size=4, embed_dim=192, depth=12, num_heads=4, mlp_ratio=4, apply_dhvt=True)\n",
    "    model_kwargs.update(kwargs)\n",
    "    model = _create_vision_transformer('deit_tiny_patch16_224', pretrained=pretrained, **model_kwargs)\n",
    "    print(model)\n",
    "    return model\n",
    "\n",
    "\n",
    "@register_model\n",
    "def dhvt_small_cifar_patch4(pretrained=False, **kwargs):\n",
    "    model_kwargs = dict(img_size=32, patch_size=4, embed_dim=384, depth=12, num_heads=8, mlp_ratio=4, apply_dhvt=True)\n",
    "    model_kwargs.update(kwargs)\n",
    "    model = _create_vision_transformer('deit_small_patch16_224', pretrained=pretrained, **model_kwargs)\n",
    "    print(model)\n",
    "    return model\n",
    "\n",
    "\n",
    "@register_model\n",
    "def dhvt_tiny_cifar_patch2(pretrained=False, **kwargs):\n",
    "    model_kwargs = dict(img_size=32, patch_size=2, embed_dim=192, depth=12, num_heads=4, mlp_ratio=4, apply_dhvt=True)\n",
    "    model_kwargs.update(kwargs)\n",
    "    model = _create_vision_transformer('deit_tiny_patch16_224', pretrained=pretrained, **model_kwargs)\n",
    "    print(model)\n",
    "    return model\n",
    "\n",
    "\n",
    "@register_model\n",
    "def dhvt_small_cifar_patch2(pretrained=False, **kwargs):\n",
    "    model_kwargs = dict(img_size=32, patch_size=2, embed_dim=384, depth=12, num_heads=8, mlp_ratio=4, apply_dhvt=True)\n",
    "    model_kwargs.update(kwargs)\n",
    "    model = _create_vision_transformer('deit_small_patch16_224', pretrained=pretrained, **model_kwargs)\n",
    "    print(model)\n",
    "    return model\n",
    "\n",
    "\n",
    "@register_model\n",
    "def dhvt_tiny_domain_patch16(pretrained=False, **kwargs):\n",
    "    model_kwargs = dict(img_size=224, patch_size=16, embed_dim=192, depth=12, num_heads=4, mlp_ratio=4, apply_dhvt=True)\n",
    "    model_kwargs.update(kwargs)\n",
    "    model = _create_vision_transformer('deit_tiny_patch16_224', pretrained=pretrained, **model_kwargs)\n",
    "    print(model)\n",
    "    return model\n",
    "\n",
    "\n",
    "@register_model\n",
    "def dhvt_small_domain_patch16(pretrained=False, **kwargs):\n",
    "    model_kwargs = dict(img_size=224, patch_size=16, embed_dim=384, depth=12, num_heads=6, mlp_ratio=4, apply_dhvt=True)\n",
    "    model_kwargs.update(kwargs)\n",
    "    model = _create_vision_transformer('deit_small_patch16_224', pretrained=pretrained, **model_kwargs)\n",
    "    print(model)\n",
    "    return model\n",
    "\n",
    "\n",
    "@register_model\n",
    "def dhvt_tiny_imagenet_patch16(pretrained=False, **kwargs):\n",
    "    model_kwargs = dict(img_size=224, patch_size=16, embed_dim=192, depth=12, num_heads=3, mlp_ratio=4, apply_dhvt=True)\n",
    "    model_kwargs.update(kwargs)\n",
    "    model = _create_vision_transformer('deit_tiny_patch16_224', pretrained=pretrained, **model_kwargs)\n",
    "    print(model)\n",
    "    return model\n",
    "\n",
    "\n",
    "@register_model\n",
    "def dhvt_small_imagenet_patch16(pretrained=False, **kwargs):\n",
    "    model_kwargs = dict(img_size=224, patch_size=16, embed_dim=384, depth=12, num_heads=6, mlp_ratio=4, apply_dhvt=True)\n",
    "    model_kwargs.update(kwargs)\n",
    "    model = _create_vision_transformer('deit_small_patch16_224', pretrained=pretrained, **model_kwargs)\n",
    "    print(model)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0811dd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_path_rate = 0.1\n",
    "depth = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8ca5662",
   "metadata": {},
   "outputs": [],
   "source": [
    " dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "630421af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 0.00909090880304575,\n",
       " 0.0181818176060915,\n",
       " 0.027272727340459824,\n",
       " 0.036363635212183,\n",
       " 0.045454543083906174,\n",
       " 0.054545458406209946,\n",
       " 0.06363636255264282,\n",
       " 0.0727272778749466,\n",
       " 0.08181818574666977,\n",
       " 0.09090909361839294,\n",
       " 0.10000000149011612]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07fb9600",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = torch.rand(1, 197, 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89d0ff13",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"DATASETS/Leather/leather/test/color/000.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5bb5fde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e2fac73e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'000.png'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.split(path)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "24345022",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 0. Expected size 3 but got size 1 for tensor number 4 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 15\u001b[0m\n\u001b[0;32m      4\u001b[0m tensor_list \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      5\u001b[0m     torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m),\n\u001b[0;32m      6\u001b[0m     torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m     torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m)\n\u001b[0;32m     12\u001b[0m ]\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Concatenate tensors along the batch dimension (dim=0)\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m concatenated_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Now 'concatenated_tensor' contains all tensors concatenated along the batch dimension\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# The shape of 'concatenated_tensor' will be (num_tensors, 3, 224, 224) in this example\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 0. Expected size 3 but got size 1 for tensor number 4 in the list."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Assuming you have a list of tensors named 'tensor_list'\n",
    "tensor_list = [\n",
    "    torch.randn(1, 3, 224, 224),\n",
    "    torch.randn(1, 3, 224, 224),\n",
    "    torch.randn(1, 3, 224, 224),\n",
    "    torch.randn(1, 3, 224, 224),\n",
    "    torch.randn(1, 1, 224, 224),\n",
    "    torch.randn(1, 1, 224, 224),\n",
    "    torch.randn(1, 1, 224, 224)\n",
    "]\n",
    "\n",
    "# Concatenate tensors along the batch dimension (dim=0)\n",
    "concatenated_tensor = torch.cat(tensor_list, dim=0)\n",
    "\n",
    "# Now 'concatenated_tensor' contains all tensors concatenated along the batch dimension\n",
    "# The shape of 'concatenated_tensor' will be (num_tensors, 3, 224, 224) in this example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "04f6fcd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "tensor = torch.rand(1, 1, 224, 224)\n",
    "three_channel_tensor = torch.cat((tensor,) * 3, dim=1)\n",
    "\n",
    "# The resulting tensor will have shape [1, 3, 224, 224]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e05e1b31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 224, 224])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "three_channel_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56dfc7cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
